{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Graph Model Correct Solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sefeoglu/OntologyMatching-Evaluation_Results/blob/master/Graph_Model_Correct_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXljo91VebPW",
        "outputId": "aa1e5d40-7c5d-4148-ae3f-f7cbdd90de64"
      },
      "source": [
        "pip install ontospy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ontospy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/99/fae085e15dd328f2b6edec4cfc1ad434a4f96d37f7a79b74591e371d23e7/ontospy-1.9.8.3-py2.py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 3.9MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ontospy) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from ontospy) (7.1.2)\n",
            "Collecting SPARQLWrapper\n",
            "  Downloading https://files.pythonhosted.org/packages/00/9b/443fbe06996c080ee9c1f01b04e2f683b2b07e149905f33a2397ee3b80a2/SPARQLWrapper-1.8.5-py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ontospy) (2.4.7)\n",
            "Collecting keepalive\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/11/8eaf21d74b06cfabd42ca9d2b7b216e071faa416753faeb3322f1863f585/keepalive-0.5.tar.gz\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.7/dist-packages (from ontospy) (1.0.1)\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 52.8MB/s \n",
            "\u001b[?25hCollecting pyfiglet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/07/fcfdd7a2872f5b348953de35acce1544dab0c1e8368dca54279b1cde5c15/pyfiglet-0.8.post1-py2.py3-none-any.whl (865kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 61.8MB/s \n",
            "\u001b[?25hCollecting rdflib-jsonld\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/17/45e137be0d93b70827fe5529c0400731344a978bc792193d7d9152e6dbe4/rdflib-jsonld-0.5.0.tar.gz (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ontospy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ontospy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ontospy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ontospy) (1.24.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from html5lib->ontospy) (1.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from html5lib->ontospy) (0.5.1)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: keepalive, rdflib-jsonld\n",
            "  Building wheel for keepalive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keepalive: filename=keepalive-0.5-cp37-none-any.whl size=8938 sha256=497ec848695f7daa57fabcebafb4074dce442b424137a2c46cb9fd7b846a7b14\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/56/d1/1f5ce7b1976ed0030f39e69f33af490745441216b762be9a69\n",
            "  Building wheel for rdflib-jsonld (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rdflib-jsonld: filename=rdflib_jsonld-0.5.0-py2.py3-none-any.whl size=15372 sha256=464d1cf7af316fdf766a0be5e7facdb888b724b33e8233ab631adbab74d917f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/e4/7f/9ebcb3e400c694e645c3adba40ef3e9bda78384ac3b9b0d13d\n",
            "Successfully built keepalive rdflib-jsonld\n",
            "Installing collected packages: colorama, isodate, rdflib, SPARQLWrapper, keepalive, pyfiglet, rdflib-jsonld, ontospy\n",
            "Successfully installed SPARQLWrapper-1.8.5 colorama-0.4.4 isodate-0.6.0 keepalive-0.5 ontospy-1.9.8.3 pyfiglet-0.8.post1 rdflib-5.0.0 rdflib-jsonld-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cI0vOpQ3RKY",
        "outputId": "f2ec6b5c-d4b5-47c8-ad90-43bd78ad9314"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmjohvDne4Um",
        "outputId": "e051d621-4126-43fe-8d70-0fefc59959bc"
      },
      "source": [
        "pip install sentence_transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence_transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/fd/8a81047bbd9fa134a3f27e12937d2a487bd49d353a038916a5d7ed4e5543/sentence-transformers-2.0.0.tar.gz (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.1MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 16.4MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/d5/07894f2f047055576c3559a66d921c37d1280fda74b08fe574a8490e5999/huggingface_hub-0.0.14-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 18.7MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 23.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.4.7)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.0.0-cp37-none-any.whl size=126711 sha256=c104db081cc885519f51e1a0f8dcbe401fa530908d07d65f1e78f83981c5bc9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/d2/98/d191289a877a34c68aa67e05179521e060f96394a3e9336be6\n",
            "Successfully built sentence-transformers\n",
            "\u001b[31mERROR: transformers 4.8.2 has requirement huggingface-hub==0.0.12, but you'll have huggingface-hub 0.0.14 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.0.14 sacremoses-0.0.45 sentence-transformers-2.0.0 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNc6Wa13tmBL"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d-2sNsxuOCu"
      },
      "source": [
        "### Ontology Parser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDCa7sdEuNli"
      },
      "source": [
        "\n",
        "import ontospy\n",
        "\n",
        "class OntologyParser(object):\n",
        "    \n",
        "    def __init__(self, rdf_file):\n",
        "        self.onto = ontospy.Ontospy(rdf_file)\n",
        "        self.classes = self.get_classes()\n",
        "        self.properties = self.get_properties()\n",
        "    \n",
        "    def get_properties(self):\n",
        "        properties = []\n",
        "        for prop in self.onto.all_properties:\n",
        "            prop = str(prop).split(\"/\")[-1].replace(\"*>\",\"\").split(\"#\")[-1]\n",
        "            properties.append(prop)\n",
        "        return properties\n",
        "   \n",
        "    def get_classes(self):\n",
        "        classes = []\n",
        "        for concept in self.onto.all_classes:\n",
        "            concept = str(concept).split(\"/\")[-1].replace(\"*>\",\"\").split(\"#\")[-1]\n",
        "            classes.append(concept)\n",
        "        return classes\n",
        "\n",
        "    def get_all_triples(self):\n",
        "\n",
        "        triples = self.onto.sparql(\"SELECT ?subject ?predicate ?object  WHERE { ?subject ?predicate ?object}\")\n",
        "        return triples\n",
        "\n",
        "    def get_inferred_properties(self, class_uri):\n",
        "        props = self.onto.getInferredPropertiesForClass(class_uri)\n",
        "        return props"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqc_L_mEuRlH"
      },
      "source": [
        "### GraphParser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQt-AndDtk_r"
      },
      "source": [
        "## preprocessing Codes\n",
        "\n",
        "import sys, configparser\n",
        "import os, itertools, re, logging, requests, urllib\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from xml.dom import minidom\n",
        "import itertools\n",
        "import numpy as np\n",
        "import logging\n",
        "from scipy import spatial\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# PACKAGE_PARENT = '.'\n",
        "# SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser(__file__))))\n",
        "# sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
        "# from OntologyParser import OntologyParser\n",
        "try:\n",
        "    # ubuntu works with USE(universal sentence encoder), otherwise it will run with bert sentence encoder\n",
        "    import tensorflow_hub as hub\n",
        "except:\n",
        "    pass\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "def cos_sim(a,b):\n",
        "    return 1 - spatial.distance.cosine(a, b)\n",
        "class GraphParser(object):\n",
        "\n",
        "    def __init__(self, ontologies_in_alignment, alignments=None):\n",
        "        ### Folder Definition ###\n",
        "\n",
        "        self.prefix_path = \"drive/MyDrive/Thesis_OM/\"  #\"/\".join(os.path.dirname(os.path.abspath(__file__)).split(\"/\")[:-2]) + \"/\"\n",
        "        config = configparser.ConfigParser()\n",
        "        config.read(self.prefix_path + 'config.ini')\n",
        "        if alignments == None:\n",
        "           self.alignments = [] \n",
        "        else:\n",
        "            self.alignments = alignments\n",
        "            \n",
        "        self.ontologies_in_alignment = ontologies_in_alignment\n",
        "        self.train_folder = self.prefix_path + str(config['Paths']['dataset_folder']) + str(config[\"General\"]['dataset'])+'/ontologies/'\n",
        "        self.clean_set = self.prefix_path + str(config[\"Paths\"][\"spreadsheet_folder\"]) + str(config[\"Paths\"][\"clean_dataset\"])\n",
        "        self.preprocessed_dataset = self.prefix_path + str(config[\"Paths\"][\"spreadsheet_folder\"]) + str(config[\"Paths\"][\"preprocessed_dataset\"])\n",
        "        self.alignment_folder = self.prefix_path + str(config['Paths']['dataset_folder']) + str(config[\"General\"]['dataset']) + str(config[\"Paths\"][\"alignment_folder\"])\n",
        "        self.output_folder_csv = self.prefix_path + str(config[\"Paths\"][\"spreadsheet_folder\"]) + str(config['Paths'][\"dataset_folder\"])\n",
        "        self.all_data_folder = self.prefix_path + str(config[\"Paths\"][\"spreadsheet_folder\"])\n",
        "        try:\n",
        "            self.USE_link = \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\"\n",
        "            self.USE = hub.load(self.USE_link)\n",
        "        except:\n",
        "            pass\n",
        "        self.model_transformer = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "        self.stopwords = [\"has\"]\n",
        "\n",
        "    def extractUSEEmbeddings(self, words):\n",
        "        try:\n",
        "            word_embeddings = self.USE(words).numpy()\n",
        "        except:\n",
        "            word_embeddings = self.model_transformer.encode(words)\n",
        "\n",
        "        return word_embeddings\n",
        "\n",
        "    def create_spreadsheet_from_triples(self):\n",
        "        \"\"\"Creates spreadsheet from triples\n",
        "        \"\"\"        \n",
        "        file_list = os.listdir(self.train_folder)\n",
        "        subject, predicate, object_ls, ns_l, ontology_list= [], [], [], [], []\n",
        "        for file_name in file_list:\n",
        "            ontology_list.append(self.train_folder+file_name)\n",
        "            rdf_file = self.train_folder+file_name\n",
        "            o_parser =  OntologyParser(rdf_file)\n",
        "            triples = o_parser.get_all_triples()\n",
        "            ns = file_name.split('.')[0]\n",
        "            for s, p, o in triples:\n",
        "                subject.append(s)\n",
        "                predicate.append(p)\n",
        "                object_ls.append(o)\n",
        "                ns_l.append(ns)\n",
        "\n",
        "        data = {'subject':subject, 'predicate':predicate, 'object': object_ls,'ns':ns_l}\n",
        "        data_frame = pd.DataFrame(data)\n",
        "\n",
        "        return data_frame\n",
        "            \n",
        "            \n",
        "    def inferred_properties(self, ontology_parser):\n",
        "\n",
        "        inferred_properties = []\n",
        "        classes = ontology_parser.get_classes()\n",
        "        for ent in classes:\n",
        "            inferred_properties.append(ontology_parser.get_inferred_properties(ent.split('#')[1]))\n",
        "\n",
        "        return inferred_properties\n",
        "\n",
        "    def preprocess_triples(self, data_frame):   \n",
        "\n",
        "        for i in range(len(data_frame)):\n",
        "            if len(str(data_frame.predicate[i]).split('#')[-1].split(\"/\")[-1].replace(\"*>\",\"\").split(\"#\")[-1])!=0:\n",
        "                clean_predicate = str(data_frame.predicate[i]).split('#')[-1].split(\"/\")[-1].replace(\"*>\",\"\").split(\"#\")[-1]\n",
        "                data_frame.predicate[i] = clean_predicate\n",
        "            if len(str(data_frame.subject[i]).split('#')[-1].split(\"/\")[-1].replace(\"*>\",\"\").split(\"#\")[-1])!=0:\n",
        "                clean_subject = str(data_frame.subject[i]).split('#')[-1].split(\"/\")[-1].replace(\"*>\",\"\").split(\"#\")[-1]\n",
        "                data_frame.subject[i] = clean_subject\n",
        "            if len(str(data_frame.object[i]).split('#')[-1].split(\"/\")[-1].replace(\"*>\",\"\").split(\"#\")[-1])!=0:\n",
        "                clean_object = str(data_frame.object[i]).split('#')[-1].split(\"/\")[-1].replace(\"*>\",\"\").split(\"#\")[-1]\n",
        "                data_frame.object[i] = clean_object\n",
        "\n",
        "        return data_frame\n",
        "\n",
        "    def construct_neighbour_dicts(self,ontologies_in_alignment):\n",
        "\n",
        "        neighbours_dicts_ent, neighbours_dicts_prop = {}, {}\n",
        "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "        data_frame = self.data_frame \n",
        "\n",
        "        for ont in list(set(flatten(ontologies_in_alignment))):\n",
        "            \n",
        "            onto_frame = data_frame[data_frame.ns == ont.split('/')[-1].split('.')[0]]\n",
        "            self.kg_df = pd.DataFrame({'source':onto_frame.subject, 'target':onto_frame.object, 'edge':onto_frame.predicate})\n",
        "            \n",
        "            neighbours_dicts_ent.update(self.entity_neighbour_dict(ont))\n",
        "\n",
        "            neighbours_dicts_prop.update(self.property_neighbour_dict(ont))\n",
        "\n",
        "        max_types = np.max([len([nbr_type for nbr_type in elem if flatten(nbr_type)]) for elem in neighbours_dicts_ent.values()])\n",
        "\n",
        "        return neighbours_dicts_ent, neighbours_dicts_prop, max_types\n",
        "\n",
        "    def entity_neighbour_dict(self, ontology):\n",
        "        self.parser = OntologyParser(ontology)\n",
        "        entities = self.parser.classes\n",
        "        neighbours_dicts_ent = {}\n",
        "        ont = str(ontology).split('/')[-1].split('.')[0]\n",
        "\n",
        "        for ent in entities:\n",
        "            subcClass_neighbour, equivalent_neighbour, domain_neighbour, range_neighbour, parents = self.one_hop_class(ent, ont)\n",
        "            ent = ont+\"#\"+str(ent).split(\"/\")[-1].replace(\"*>\",\"\").split(\"#\")[-1]\n",
        "            neighbours_dicts_ent[ent] = [[parents], subcClass_neighbour, domain_neighbour, range_neighbour, equivalent_neighbour]\n",
        "\n",
        "        return neighbours_dicts_ent\n",
        "\n",
        "    def property_neighbour_dict(self, ontology):\n",
        "        \n",
        "        parser = OntologyParser(ontology)\n",
        "        properties = parser.properties\n",
        "        neighbours_dicts_prop = {}\n",
        "        ont = str(ontology).split('/')[-1].split('.')[0]\n",
        "\n",
        "        for prop in properties:\n",
        "            domain, _range, subprop, inverse_prop = self.one_hop_property(prop, ont)\n",
        "            prop_node = ont+\"#\"+str(prop).split(\"/\")[-1].replace(\"*>\",\"\").split(\"#\")[-1]\n",
        "            prop = ont+\"#\"+str(prop).split(\"/\")[-1].replace(\"*>\",\"\").split(\"#\")[-1]\n",
        "            neighbours_dicts_prop[prop]=[[prop_node], domain, _range, subprop, inverse_prop]\n",
        "\n",
        "        return neighbours_dicts_prop\n",
        "\n",
        "    def one_hop_property(self, prop, ont, attention_type='two'):\n",
        "        # get onProperty\n",
        "        prop_range, prop_domain = [], []\n",
        "        somevalues_df = self.kg_df[self.kg_df['edge']=='someValuesFrom'] \n",
        "        unionOf_df = self.kg_df[self.kg_df['edge']=='unionOf']\n",
        "        oneof_df = self.kg_df[self.kg_df['edge']=='oneOf']\n",
        "        allValuesFrom_df = self.kg_df[self.kg_df['edge']=='allValuesFrom']\n",
        "        first_df = self.kg_df[self.kg_df['edge']=='first']\n",
        "        rest_df = self.kg_df[self.kg_df['edge']=='rest']\n",
        "        domain_df = self.kg_df[self.kg_df['edge']== 'domain']\n",
        "        range_df = self.kg_df[self.kg_df['edge']== 'range']\n",
        "       \n",
        "        G = self.create_graph(domain_df)\n",
        "        prop = prop.split('#')[-1]\n",
        "        ### Domain of Property\n",
        "        try:\n",
        "            props =  nx.neighbors(G, prop)\n",
        "            for p in props:\n",
        "                if str(p).startswith('N'):\n",
        "                    prop_domain = self.unionOf(p, unionOf_df, first_df, rest_df, prop_domain, ont)\n",
        "                    prop_domain = self.Restriction(p, somevalues_df, first_df, rest_df, unionOf_df, prop_domain, ont)\n",
        "                    prop_domain = self.Restriction(p, oneof_df, first_df, rest_df, unionOf_df, prop_domain, ont)\n",
        "                    prop_domain = self.Restriction(p, allValuesFrom_df, first_df, rest_df, unionOf_df, prop_domain, ont)\n",
        "                else:\n",
        "                    prop_domain.append([ont+\"#\"+p])\n",
        "        except:\n",
        "            pass\n",
        "        ### Range of Property###\n",
        "        G = self.create_graph(range_df)\n",
        "        try:\n",
        "            props =  nx.neighbors(G, prop)\n",
        "            for p in props:\n",
        "                if str(p).startswith('N'):\n",
        "                    prop_range = self.unionOf(p, unionOf_df, first_df, rest_df, prop_range, ont)\n",
        "                    prop_range = self.Restriction(p, somevalues_df, first_df, rest_df, unionOf_df, prop_range, ont)\n",
        "                    prop_range = self.Restriction(p, oneof_df, first_df, rest_df, unionOf_df, prop_range, ont)\n",
        "                    prop_range = self.Restriction(p, allValuesFrom_df, first_df, rest_df, unionOf_df, prop_range, ont)\n",
        "                else:\n",
        "                    prop_range.append([ont+\"#\"+p])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        ## For subprop\n",
        "        subprop, inverse_prop = [], []\n",
        "        if attention_type == 'two':\n",
        "            subprop, inverse_prop = self.two_hop_property(prop, subprop, inverse_prop, ont)\n",
        "        \n",
        "        if len(prop_domain) != 0:\n",
        "            prop_domain =  prop_domain[0]\n",
        "        if len(prop_range) != 0:\n",
        "            prop_range =  prop_range[0]\n",
        "\n",
        "        if len(subprop) != 0:\n",
        "            subprop =  subprop[0]\n",
        "        if len(inverse_prop) != 0:\n",
        "            inverse_prop =  inverse_prop[0]\n",
        "\n",
        "        return prop_domain, prop_range, subprop, inverse_prop\n",
        "\n",
        "    def create_graph(self, data_frame):\n",
        "        G=nx.from_pandas_edgelist(data_frame, \"source\", \"target\", \n",
        "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
        "        G = nx.to_undirected(G)\n",
        "        return G\n",
        "\n",
        "    def Restriction(self,node, main_df,first_df, rest_df, union_df, class_neighbour, ont):\n",
        "        try:\n",
        "            G = self.create_graph(main_df)\n",
        "            allnodes = nx.neighbors(G, node)\n",
        "            for anode in allnodes:\n",
        "                if str(anode).startswith('N'):\n",
        "                    class_neighbour = self.unionOf(anode,union_df,first_df, rest_df, class_neighbour, ont)\n",
        "                    try:\n",
        "                        G = self.create_graph(first_df)\n",
        "                        first_nodes = nx.neighbors(G, anode)\n",
        "                        for fn in first_nodes:\n",
        "                            if str(fn).startswith('N'):\n",
        "                                continue\n",
        "                            else:\n",
        "                                class_neighbour.append([ont+\"#\"+fn])\n",
        "                        G = self.create_graph(rest_df)\n",
        "                        rest_nodes = nx.neighbors(G, anode)\n",
        "                        for rn in rest_nodes:\n",
        "                            if str(rn).startswith('N'):\n",
        "                                try:\n",
        "                                    G = self.create_graph(first_df)\n",
        "                                    fnodes2 = nx.neighbors(G, rn)\n",
        "                                    for fn2 in fnodes2:\n",
        "                                        class_neighbour.append([ont+\"#\"+fn2])\n",
        "                                except:\n",
        "                                    pass\n",
        "                            else:\n",
        "                                class_neighbour.append([ont+\"#\"+fn])\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                else:\n",
        "                    class_neighbour.append([ont+\"#\"+anode])\n",
        "        except:\n",
        "            pass\n",
        "        return class_neighbour\n",
        "\n",
        "    def unionOf(self, node, unionOf_df, first_df,rest_df, class_neighbour, ont):\n",
        "        try:\n",
        "            G = self.create_graph(unionOf_df)\n",
        "            unodes = nx.neighbors(G, node)\n",
        "            for unode in unodes:\n",
        "                if str(unode).startswith('N'):\n",
        "                    try:\n",
        "                        G = self.create_graph(first_df)\n",
        "                        first_nodes = nx.neighbors(G, unode)\n",
        "                        for fn in first_nodes:\n",
        "                            if str(fn).startswith('N'):\n",
        "                                continue\n",
        "                            else:\n",
        "                                class_neighbour.append([ont+\"#\"+fn])\n",
        "                        G = self.create_graph(rest_df)\n",
        "                        rest_nodes = nx.neighbors(G, unode)\n",
        "                        for rn in rest_nodes:\n",
        "                            if str(rn).startswith('N'):\n",
        "                                try:\n",
        "                                    G = self.create_graph(first_df)\n",
        "                                    fnodes2 = nx.neighbors(G, rn)\n",
        "                                    for fn2 in fnodes2:\n",
        "                                        class_neighbour.append([ont+\"#\"+fn2])\n",
        "                                    # Rest of Rest\n",
        "                                    G = self.create_graph(rest_df)\n",
        "                                    rnodes2 = nx.neighbors(G, rn)\n",
        "                                    for rn2 in rnodes2:\n",
        "                                        if str(rn2).startswith('N'):\n",
        "                                            try:\n",
        "                                                G = self.create_graph(first_df)\n",
        "                                                fnodes3 = nx.neighbors(G, rn2)\n",
        "                                                for fn3 in fnodes3:\n",
        "                                                    class_neighbour.append([ont+\"#\"+fn3])\n",
        "                                            except:\n",
        "                                                pass\n",
        "                                        else:\n",
        "                                            class_neighbour.append([ont+\"#\"+rn2])\n",
        "                                except:\n",
        "                                    pass\n",
        "                            else:\n",
        "                                class_neighbour.append([ont+\"#\"+fn])\n",
        "                    except:\n",
        "                        pass\n",
        "                else:\n",
        "                    class_neighbour.append([ont+\"#\"+unode])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return class_neighbour\n",
        "\n",
        "    def onProperty(self, rsnode, onProperty_df, class_neighbour, ont):\n",
        "\n",
        "        try:\n",
        "            G = self.create_graph(onProperty_df)\n",
        "            nodes = nx.neighbors(G, rsnode)\n",
        "            for node in nodes:\n",
        "                class_neighbour.append([ont+\"#\"+node])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return class_neighbour\n",
        "\n",
        "    def range_of_prop(self, prop_node, range_df, class_neighbour, ont):\n",
        "        try:\n",
        "            G = self.create_graph(range_df)\n",
        "            nodes = nx.neighbors(G, prop_node)\n",
        "            for node in nodes:\n",
        "                class_neighbour.append([ont+\"#\"+node])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return class_neighbour\n",
        "    \n",
        "    def one_hop_class(self, entity, ont):\n",
        "        ## get subClassOf and equivalentClass\n",
        "        class_df = self.kg_df[self.kg_df['edge']=='subClassOf'] \n",
        "        equivalent_df = self.kg_df[self.kg_df['edge']=='equivalentClass']\n",
        "        somevalues_df = self.kg_df[self.kg_df['edge']=='someValuesFrom'] \n",
        "        unionOf_df = self.kg_df[self.kg_df['edge']=='unionOf']\n",
        "        oneof_df = self.kg_df[self.kg_df['edge']=='oneOf']\n",
        "        allValuesFrom_df = self.kg_df[self.kg_df['edge']=='allValuesFrom']\n",
        "        first_df = self.kg_df[self.kg_df['edge']=='first']\n",
        "        rest_df = self.kg_df[self.kg_df['edge']=='rest']\n",
        "        domain_df = self.kg_df[self.kg_df['edge']== 'domain']\n",
        "        onProperty_df = self.kg_df[self.kg_df['edge']=='onProperty']\n",
        "        range_df = self.kg_df[self.kg_df['edge']=='range']\n",
        "        \n",
        "        G = self.create_graph(class_df)\n",
        "        subcClass_neighbour, domain_neighbour, equivalent_neighbour, range_neighbour = [], [], [], []\n",
        "        ### subClassOf for children ###\n",
        "        try:\n",
        "            nodes = nx.neighbors(G, entity)\n",
        "            for node in nodes:\n",
        "                if str(node).startswith('N'):\n",
        "                    subcClass_neighbour = self.unionOf(node, unionOf_df, first_df, rest_df, subcClass_neighbour, ont)\n",
        "                    subcClass_neighbour = self.Restriction(node, somevalues_df, first_df, rest_df, unionOf_df, subcClass_neighbour, ont)\n",
        "                    subcClass_neighbour = self.Restriction(node, oneof_df, first_df, rest_df, unionOf_df, subcClass_neighbour, ont)\n",
        "                    subcClass_neighbour = self.Restriction(node, allValuesFrom_df, first_df, rest_df, unionOf_df, subcClass_neighbour, ont)\n",
        "                    subcClass_neighbour = self.onProperty(node, onProperty_df, subcClass_neighbour, ont)\n",
        "                else:\n",
        "                    subcClass_neighbour.append([ont+\"#\"+node])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        ## path to root\n",
        "        G=nx.from_pandas_edgelist(class_df, \"source\", \"target\", \n",
        "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
        "        all_parents = self.parser.onto.toplayer_classes\n",
        "        parents = [] \n",
        "        for parent in all_parents:\n",
        "            parent = str(parent).split(\"*\")[1].replace(\"*>\",\"\").split(\"#\")[-1].split(\"/\")[-1]\n",
        "            try:\n",
        "                nodes = nx.astar_path(G, entity, parent)\n",
        "                for node in nodes:\n",
        "                    if str(node).startswith('N'):\n",
        "                        parents = self.unionOf(node, unionOf_df, first_df, rest_df, parents, ont)\n",
        "                        parents = self.Restriction(node, somevalues_df, first_df, rest_df, unionOf_df, parents, ont)\n",
        "                        parents = self.Restriction(node, oneof_df, first_df, rest_df, unionOf_df, parents, ont)\n",
        "                        parents = self.Restriction(node, allValuesFrom_df, first_df, rest_df, unionOf_df, parents, ont)\n",
        "                        parents = self.onProperty(node, onProperty_df, parents, ont)\n",
        "                    else:\n",
        "                        parents.append([ont+\"#\"+node])\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        if len(parents)!= 0:\n",
        "            parent_formatted = []\n",
        "            for parent in parents:\n",
        "                parent_formatted.append(parent[0])\n",
        "            parents = parent_formatted\n",
        "        ## remove the parent class from children class\n",
        "        for child_class in subcClass_neighbour:\n",
        "            child_str = child_class[0]\n",
        "            for parent in parents:\n",
        "                if child_str == parent and len(subcClass_neighbour) != 0:\n",
        "                    subcClass_neighbour.remove(child_class)\n",
        "                    \n",
        "        ####equivalentClass\n",
        "        G = self.create_graph(equivalent_df)\n",
        "        try:\n",
        "            nodes = nx.neighbors(G, entity)\n",
        "            for node in nodes:\n",
        "                if str(node).startswith('N'):\n",
        "                    equivalent_neighbour = self.unionOf(node, unionOf_df, first_df, rest_df, equivalent_neighbour, ont)\n",
        "                    equivalent_neighbour = self.Restriction(node, somevalues_df, first_df, rest_df, unionOf_df, equivalent_neighbour, ont)\n",
        "                    equivalent_neighbour = self.Restriction(node, oneof_df, first_df, rest_df, unionOf_df, equivalent_neighbour, ont)\n",
        "                    equivalent_neighbour = self.Restriction(node, allValuesFrom_df, first_df, rest_df, unionOf_df, equivalent_neighbour, ont)\n",
        "                    ## onProperty\n",
        "                    equivalent_neighbour = self.onProperty(node, onProperty_df, equivalent_neighbour, ont)\n",
        "                else:\n",
        "                    equivalent_neighbour.append([ont+\"#\"+node])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        ## Domain and Range for props\n",
        "        G = self.create_graph(range_df)\n",
        "        try:\n",
        "            nodes = nx.neighbors(G, entity)\n",
        "            for node in nodes:\n",
        "                range_neighbour.append([ont+\"#\"+node])\n",
        "                try:\n",
        "                    G = self.create_graph(domain_df)\n",
        "                    domain_nodes = nx.neighbors(G, node)\n",
        "                    for dn in domain_nodes:\n",
        "                        if str(dn).startswith('N'):\n",
        "                            range_neighbour = self.unionOf(dn, unionOf_df, first_df, rest_df, range_neighbour, ont) \n",
        "                            range_neighbour = self.Restriction(dn, somevalues_df, first_df, rest_df, unionOf_df, range_neighbour, ont)\n",
        "                            range_neighbour = self.Restriction(dn, oneof_df, first_df, rest_df, unionOf_df, range_neighbour, ont)\n",
        "                            range_neighbour = self.Restriction(dn, allValuesFrom_df, first_df, rest_df, unionOf_df, range_neighbour, ont)\n",
        "                        else:\n",
        "                            range_neighbour.append([ont+\"#\"+dn])\n",
        "                except:\n",
        "                    pass\n",
        "                #find range of node\n",
        "        except:\n",
        "            pass\n",
        "        ## domain probs\n",
        "         \n",
        "        G = self.create_graph(domain_df)\n",
        "        try:\n",
        "            nodes = nx.neighbors(G, entity)\n",
        "            for node in nodes:\n",
        "                domain_neighbour.append([ont+\"#\"+node])\n",
        "                try:\n",
        "                    G = self.create_graph(range_df)\n",
        "                    range_nodes = nx.neighbors(G, node)\n",
        "                    for rn in range_nodes:\n",
        "                        if str(rn).startswith(\"N\"):\n",
        "                            domain_neighbour = self.unionOf(rn, unionOf_df, first_df, rest_df, domain_neighbour, ont)\n",
        "                            domain_neighbour = self.Restriction(rn, somevalues_df, first_df, rest_df, unionOf_df, domain_neighbour, ont)\n",
        "                            domain_neighbour = self.Restriction(rn, oneof_df, first_df, rest_df, unionOf_df, domain_neighbour, ont)\n",
        "                            domain_neighbour = self.Restriction(rn, allValuesFrom_df, first_df, rest_df, unionOf_df, domain_neighbour, ont)\n",
        "                        else:\n",
        "                            domain_neighbour.append([ont+\"#\"+rn])\n",
        "                except:\n",
        "                    pass\n",
        "                #find range of node\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        return subcClass_neighbour, equivalent_neighbour, domain_neighbour, range_neighbour, parents\n",
        "\n",
        "    def two_hop_property(self, prop_node, subprop, inverse_prop, ont):\n",
        "\n",
        "        subProperty_df = self.kg_df[self.kg_df['edge']== 'subPropertyOf']\n",
        "        inverse_df = self.kg_df[self.kg_df['edge']== 'inverseOf']\n",
        "        ##subPropertyOf\n",
        "        try:\n",
        "            G = self.create_graph(subProperty_df)\n",
        "            nodes = nx.neighbors(G, prop_node)\n",
        "            for node in nodes:\n",
        "                subprop.append([ont+\"#\"+node])\n",
        "        except:\n",
        "            pass\n",
        "        ### inverseOf\n",
        "        try:\n",
        "            G = self.create_graph(inverse_df)\n",
        "            nodes = nx.neighbors(G, prop_node)\n",
        "            for node in nodes:\n",
        "                inverse_prop.append([ont+\"#\"+node])\n",
        "        except:\n",
        "            pass\n",
        "        return subprop, inverse_prop\n",
        "\n",
        "    def cleaning_ontology_elements(self, data_frame):\n",
        "        \"\"\" Clean data from ontology elements without semantic relations regarding concepts\n",
        "        \"\"\"        \n",
        "        full_data = self.object_cleanup(data_frame)\n",
        "        full_data = self.predicate_cleanup(full_data)\n",
        "        full_data.drop_duplicates(keep='first',inplace=True) \n",
        "\n",
        "        return full_data\n",
        "\n",
        "    def object_cleanup(self, full_data):\n",
        "        \n",
        "        full_data = full_data[full_data.object.str.startswith('nil') == False]\n",
        "        full_data = full_data[full_data.object.str.startswith('All') == False]\n",
        "        full_data = full_data[full_data.object.str.startswith('Ontology') == False]\n",
        "        full_data = full_data[full_data.object.str.startswith('Thing') == False]\n",
        "        # Restriction\n",
        "        full_data = full_data[full_data.object.str.startswith('Rest') == False]\n",
        "        \n",
        "        return full_data\n",
        "\n",
        "    def predicate_cleanup(self,full_data ):\n",
        "        predicate_eliminator = ['cardinality', 'comment','complementOf','maxCardinality', 'minCardinality',\n",
        "                       'qualifiedCardinality','versionInfo', 'hasValue', 'disjointWith']\n",
        "        clean_full_data = full_data\n",
        "        for eliminator in predicate_eliminator:\n",
        "            clean_full_data = clean_full_data[clean_full_data.predicate != eliminator]\n",
        "        # There is a problem regarding hasValue.\n",
        "        clean_full_data = clean_full_data[clean_full_data.predicate != 'hasValue']\n",
        "        return clean_full_data\n",
        "\n",
        "    def cleaning_process(self):\n",
        "        data_frame = self.create_spreadsheet_from_triples()\n",
        "        data_frame = self.preprocess_triples(data_frame)\n",
        "        self.data_frame = self.cleaning_ontology_elements(data_frame)\n",
        "\n",
        "        return self.data_frame\n",
        "\n",
        "    def generate_mappings(self, ontologies_in_alignment, gt_mappings):\n",
        "        \n",
        "        ent_mappings, prop_mappings = [], []\n",
        "\n",
        "        for l in ontologies_in_alignment:\n",
        "\n",
        "            ont1 = l[0]\n",
        "            ont2 = l[1]\n",
        "            parser1 = OntologyParser(ont1)\n",
        "            parser2 = OntologyParser(ont2)\n",
        "            ent1 = parser1.classes\n",
        "            ent2 = parser2.classes\n",
        "            prop1 = parser1.properties\n",
        "            prop2 = parser2.properties\n",
        "            ent_mapping = list(itertools.product(ent1, ent2))\n",
        "\n",
        "            prop_mapping = list(itertools.product(prop1, prop2))\n",
        "\n",
        "            pre1 = l[0].split(\"/\")[-1].rsplit(\".\",1)[0].replace(\"-\", \"_\")\n",
        "            pre2 = l[1].split(\"/\")[-1].rsplit(\".\",1)[0].replace(\"-\", \"_\")\n",
        "\n",
        "            ent_mappings.extend([(str(pre1) + \"#\" + str(el[0]), str(pre2) + \"#\" + str(el[1])) for el in ent_mapping])\n",
        "            prop_mappings.extend([(str(pre1) + \"#\" + str(el[0]), str(pre2) + \"#\" + str(el[1])) for el in prop_mapping])\n",
        "\n",
        "        if gt_mappings:\n",
        "            data_ent = {mapping: False for mapping in ent_mappings}\n",
        "            data_prop = {mapping: False for mapping in prop_mappings}\n",
        "            s_ent = set(ent_mappings)\n",
        "            s_prop = set(prop_mappings)\n",
        "\n",
        "            for mapping in set(gt_mappings):\n",
        "                if mapping in s_ent:\n",
        "                    data_ent[mapping] = True\n",
        "                elif mapping in s_prop:\n",
        "                    data_prop[mapping] = True\n",
        "                else:\n",
        "                    mapping = tuple([el.replace(\",-\", \"_\") for el in mapping])\n",
        "                    if mapping in s_ent:\n",
        "                        data_ent[mapping] = True\n",
        "                    elif mapping in s_prop:\n",
        "                        data_prop[mapping] = True\n",
        "                    else:\n",
        "                        logging.info (\"Warning: {} given in alignments could not be found in source/target ontology.\".format(mapping))\n",
        "                        continue\n",
        "            return (data_ent, data_prop)\n",
        "\n",
        "        return (ent_mappings, prop_mappings)\n",
        "\n",
        "    def run_abbreviation_resolution(self, inp, filtered_dict):\n",
        "        # Resolving abbreviations to full forms\n",
        "        logging.info (\"Resolving abbreviations...\")\n",
        "        inp_resolved = []\n",
        "        for concept in inp:\n",
        "            for key in filtered_dict:\n",
        "                concept = concept.replace(key, filtered_dict[key])\n",
        "            final_list = []\n",
        "            # Lowering case except in abbreviations\n",
        "            for word in concept.split(\" \"):\n",
        "                if not re.search(\"[A-Z][A-Z]+\", word):\n",
        "                    final_list.append(word.lower())\n",
        "                else:\n",
        "                    final_list.append(word)\n",
        "            concept = \" \".join(final_list)\n",
        "            inp_resolved.append(concept)\n",
        "\n",
        "        return inp_resolved\n",
        "\n",
        "    def run_spellcheck(self, inp_resolved):\n",
        "        # Spelling checker and corrector\n",
        "        logging.info (\"Running spellcheck...\")\n",
        "\n",
        "        url = \"https://grammarbot.p.rapidapi.com/check\"\n",
        "\n",
        "        headers = {\n",
        "            'x-rapidapi-host': \"grammarbot.p.rapidapi.com\",\n",
        "            'x-rapidapi-key': \"9965b01207msh06291e57d6f2c55p1a6a16jsn0fb016da4a62\",\n",
        "            'content-type': \"application/x-www-form-urlencoded\"\n",
        "            }\n",
        "\n",
        "        inp_spellchecked = []\n",
        "\n",
        "        for concept in inp_resolved:\n",
        "            payload = \"language=en-US&text=\" + urllib.parse.quote_plus(concept)\n",
        "            response = requests.request(\"POST\", url, data=payload, headers=headers).json()\n",
        "            concept_corrected = str(concept)\n",
        "            \n",
        "            for elem in response[\"matches\"]:\n",
        "                start, end = elem[\"offset\"], elem[\"offset\"] + elem[\"length\"]\n",
        "                concept_corrected = concept_corrected[:start] + elem[\"replacements\"][0][\"value\"] + concept_corrected[end:]\n",
        "            \n",
        "            if concept.lower() != concept_corrected.lower():\n",
        "                logging.info (\"{} corrected to {}\".format(concept, concept_corrected))\n",
        "                inp_spellchecked.append(concept_corrected)\n",
        "            else:\n",
        "                inp_spellchecked.append(concept)\n",
        "\n",
        "        return inp_spellchecked\n",
        "\n",
        "\n",
        "    def construct_abbreviation_resolution_dict(self, all_mappings):\n",
        "        # Constructs an abbrevation resolution dict\n",
        "        logging.info (\"Constructing abbrevation resolution dict....\")\n",
        "    \n",
        "        final_dict = {}\n",
        "        for mapping in all_mappings:\n",
        "            mapping = tuple([ str(el).split(\"#\")[1] for el in mapping])\n",
        "            is_abb = re.search(\"[A-Z][A-Z]+\", mapping[0])\n",
        "            if is_abb:\n",
        "                abbreviation = \"\".join([el[0].upper() for el in mapping[1].split(\"_\")])\n",
        "                if is_abb.group() in abbreviation:\n",
        "                    \n",
        "                    start = abbreviation.find(is_abb.group())\n",
        "                    end = start + len(is_abb.group())\n",
        "                    fullform = \"_\".join(mapping[1].split(\"_\")[start:end])\n",
        "                    \n",
        "                    rest_first = \" \".join([el for el in mapping[0].replace(is_abb.group(), \"\").split(\"_\") if el]).lower()\n",
        "                    rest_second = \" \".join(mapping[1].split(\"_\")[:start] + mapping[1].split(\"_\")[end:])\n",
        "                    if is_abb.group() not in final_dict:\n",
        "                        final_dict[is_abb.group()] = [(fullform, rest_first, rest_second)]\n",
        "                    else:\n",
        "                        final_dict[is_abb.group()].append((fullform, rest_first, rest_second))\n",
        "\n",
        "            is_abb = re.search(\"[A-Z][A-Z]+\", mapping[1])\n",
        "            if is_abb:\n",
        "                abbreviation = \"\".join([el[0].upper() for el in mapping[0].split(\"_\")])\n",
        "                \n",
        "                if is_abb.group() in abbreviation:\n",
        "                    start = abbreviation.find(is_abb.group())\n",
        "                    end = start + len(is_abb.group())\n",
        "                    fullform = \"_\".join(mapping[0].split(\"_\")[start:end])\n",
        "\n",
        "                    rest_first = \" \".join([el for el in mapping[1].replace(is_abb.group(), \"\").split(\"_\") if el]).lower()\n",
        "                    rest_second = \" \".join(mapping[0].split(\"_\")[:start] + mapping[0].split(\"_\")[end:])\n",
        "                    if is_abb.group() not in final_dict:\n",
        "                        final_dict[is_abb.group()] = [(fullform, rest_first, rest_second)]\n",
        "                    else:\n",
        "                        final_dict[is_abb.group()].append((fullform, rest_first, rest_second))\n",
        "\n",
        "        keys = [el for el in list(set(flatten([flatten([tup[1:] for tup in final_dict[key]]) for key in final_dict]))) if el]\n",
        "        abb_embeds = dict(zip(keys, self.extractUSEEmbeddings(keys)))\n",
        "\n",
        "        scored_dict = {}\n",
        "        for abbr in final_dict:\n",
        "            sim_list = [(tup[0], tup[1], tup[2], cos_sim(abb_embeds[tup[1]], abb_embeds[tup[2]])) if tup[1] and tup[2]\n",
        "                        else (tup[0], tup[1], tup[2], 0) for tup in final_dict[abbr]]\n",
        "            scored_dict[abbr] = sorted(list(set(sim_list)), key=lambda x:x[-1], reverse=True)\n",
        "\n",
        "        resolved_dict = {key: scored_dict[key][0] for key in scored_dict}\n",
        "        filtered_dict = {key: \" \".join(resolved_dict[key][0].split(\"_\")) for key in resolved_dict if resolved_dict[key][-1] > 0.9}\n",
        "        logging.info (\"Results after abbreviation resolution: \", filtered_dict)\n",
        "        return filtered_dict\n",
        "\n",
        "    def camel_case_split(self, identifier):\n",
        "        # Splits camel case strings\n",
        "        matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
        "        return [m.group(0) for m in matches]\n",
        "\n",
        "    def parse(self, word):\n",
        "        return \" \".join(flatten([el.split(\"_\") for el in self.camel_case_split(word)]))\n",
        "\n",
        "    def extract_keys(self, ontologies_in_alignment):\n",
        "\n",
        "        extracted_elems = []\n",
        "        data_frame = self.data_frame \n",
        "        for ont_name in list(set(flatten(ontologies_in_alignment))):\n",
        "            ont_name_filt = ont_name.split(\"/\")[-1].rsplit(\".\",1)[0].replace(\"-\", \"_\")\n",
        "            onto_frame = data_frame[data_frame.ns == ont_name_filt]\n",
        "            self.predicate = list(onto_frame[\"predicate\"])\n",
        "            self.subject = list(onto_frame[\"subject\"])\n",
        "            self.object =list(onto_frame[\"object\"])\n",
        "            extracted_elems.extend([ont_name_filt + \"#\" + elem for elem in self.subject + self.predicate + self.object])\n",
        "        extracted_elems = list(set(extracted_elems))\n",
        "        inp = []\n",
        "        for word in extracted_elems:\n",
        "            ont_name = word.split(\"#\")[0]\n",
        "            elem = word.split(\"#\")[1]\n",
        "            inp.append(self.parse(elem))\n",
        "\n",
        "        logging.info (\"Total number of extracted unique classes and properties from entire RA set: \", len(extracted_elems))\n",
        "\n",
        "        extracted_elems = [\"<UNK>\"] + extracted_elems\n",
        "        return inp, extracted_elems\n",
        "\n",
        "\n",
        "    def extract_embeddings(self, inp, extracted_elems):\n",
        "\n",
        "        # Creates embeddings to index dict, word to index dict etc\n",
        "        embeds = np.array(self.extractUSEEmbeddings(inp))\n",
        "        embeds = np.array([np.zeros(embeds.shape[1],)] + list(embeds))\n",
        "        embeddings = dict(zip(extracted_elems, embeds))\n",
        "\n",
        "        emb_vals = list(embeddings.values())\n",
        "        emb_indexer = {key: i for i, key in enumerate(list(embeddings.keys()))}\n",
        "        emb_indexer_inv = {i: key for i, key in enumerate(list(embeddings.keys()))}\n",
        "\n",
        "        return emb_vals, emb_indexer, emb_indexer_inv\n",
        "\n",
        "    def remove_stopwords(self, inp):\n",
        "        # Remove high frequency stopwords\n",
        "        inp_filtered = []\n",
        "        for elem in inp:\n",
        "            words = \" \".join([word for word in elem.split() if word not in self.stopwords])\n",
        "            words = words.replace(\"-\", \" \")\n",
        "            inp_filtered.append(words)\n",
        "        return inp_filtered\n",
        "\n",
        "    def process(self, spellcheck=False):\n",
        "        self.data_frame = self.cleaning_process()\n",
        "        gt_mappings = [tuple([elem.split(\"/\")[-1] for elem in el]) for el in self.alignments]\n",
        "        gt_mappings = [tuple([el.split(\"#\")[0].rsplit(\".\", 1)[0] +  \"#\" +  el.split(\"#\")[1] for el in tup]) for tup in gt_mappings]\n",
        "\n",
        "        ent_mappings, prop_mappings = self.generate_mappings(self.ontologies_in_alignment, gt_mappings) #OK\n",
        "        \n",
        "        inp, extracted_elems = self.extract_keys(self.ontologies_in_alignment)\n",
        "        filtered_dict = self.construct_abbreviation_resolution_dict(list(ent_mappings) + list(prop_mappings))\n",
        "\n",
        "        inp_resolved = self.run_abbreviation_resolution(inp, filtered_dict)\n",
        "        if spellcheck:\n",
        "            try:\n",
        "                inp_resolved = self.run_spellcheck(inp_resolved)\n",
        "            except:\n",
        "                pass\n",
        "        inp = self.remove_stopwords(inp_resolved)\n",
        "        emb_vals, emb_indexer, emb_indexer_inv = self.extract_embeddings(inp, extracted_elems)\n",
        "        neighbours_dicts_ent, neighbours_dicts_prop, max_types = self.construct_neighbour_dicts(self.ontologies_in_alignment)\n",
        "   \n",
        "        return  ent_mappings, prop_mappings, emb_indexer, emb_indexer_inv, emb_vals, neighbours_dicts_ent, neighbours_dicts_prop, max_types\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djgttv48trw0"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34GtiiHMd9l7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SemanticAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, in_size, hidden_size=768):\n",
        "\n",
        "        super(SemanticAttention, self).__init__()\n",
        "\n",
        "        self.project = nn.Sequential(\n",
        "            nn.Linear(in_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "\n",
        "        w = self.project(z).mean(0)                 # (M, 1)\n",
        "        beta = torch.softmax(w, dim=0)                 # (M, 1)\n",
        "        beta = beta.expand((z.shape[0],) + beta.shape) # (N, M, 1)\n",
        "\n",
        "        result =  (beta * z)                     # (N, D * K)\n",
        "        return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cby30o5ytriT"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "## Pytorch Graph Attention Network\n",
        "### the original codes here: https://github.com/Diego999/pyGAT and we exploited from their codes.\n",
        "#This is a pytorch implementation of the Graph Attention Network (GAT) model presented by Veličković et. al (2017, https://arxiv.org/abs/1710.10903).\n",
        "\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, h, adj):\n",
        "        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
        "        a_input = self._prepare_attentional_mechanism_input(Wh)\n",
        "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        adj = torch.DoubleTensor(adj).to(self.device)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "\n",
        "\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "\n",
        "        h_prime = torch.matmul(attention, Wh)\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "    def _prepare_attentional_mechanism_input(self, Wh):\n",
        "        N = Wh.size()[0] # number of nodes\n",
        "\n",
        "        # Below, two matrices are created that contain embeddings in their rows in different orders.\n",
        "        # (e stands for embedding)\n",
        "        # These are the rows of the first matrix (Wh_repeated_in_chunks): \n",
        "        # e1, e1, ..., e1,            e2, e2, ..., e2,            ..., eN, eN, ..., eN\n",
        "        # '-------------' -> N times  '-------------' -> N times       '-------------' -> N times\n",
        "        # \n",
        "        # These are the rows of the second matrix (Wh_repeated_alternating): \n",
        "        # e1, e2, ..., eN, e1, e2, ..., eN, ..., e1, e2, ..., eN \n",
        "        # '----------------------------------------------------' -> N times\n",
        "        # \n",
        "        \n",
        "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=0)\n",
        "        Wh_repeated_alternating = Wh.repeat(N, 1)\n",
        "        # Wh_repeated_in_chunks.shape == Wh_repeated_alternating.shape == (N * N, out_features)\n",
        "\n",
        "        # The all_combination_matrix, created below, will look like this (|| denotes concatenation):\n",
        "        # e1 || e1\n",
        "        # e1 || e2\n",
        "        # e1 || e3\n",
        "        # ...\n",
        "        # e1 || eN\n",
        "        # e2 || e1\n",
        "        # e2 || e2\n",
        "        # e2 || e3\n",
        "        # ...\n",
        "        # e2 || eN\n",
        "        # ...\n",
        "        # eN || e1\n",
        "        # eN || e2\n",
        "        # eN || e3\n",
        "        # ...\n",
        "        # eN || eN\n",
        "\n",
        "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\n",
        "        # all_combinations_matrix.shape == (N * N, 2 * out_features)\n",
        "\n",
        "        return all_combinations_matrix.view(N, N, 2 * self.out_features)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqJZkPn6tt0M"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os, sys\n",
        "import numpy as np\n",
        "\n",
        "# PACKAGE_PARENT = '.'\n",
        "# SCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser(__file__))))\n",
        "# sys.path.append(os.path.normpath(os.path.join(SCRIPT_DIR, PACKAGE_PARENT)))\n",
        "\n",
        "\n",
        "# from graph_attention import GraphAttentionLayer as GATConv\n",
        "\n",
        "\n",
        "class HANLayer(nn.Module):\n",
        " \n",
        "    def __init__(self, in_size, out_size, dropout):\n",
        "        super().__init__()\n",
        "        self.alpha = 0.2\n",
        "        self.gat = GraphAttentionLayer(in_size, out_size, dropout, alpha=self.alpha, concat=True)\n",
        "        self.semantic_attention = SemanticAttention(in_size, out_size)\n",
        "        \n",
        "    def forward(self, hs, adj):\n",
        "        \n",
        "        semantic_embeddings = []\n",
        "        for i, h in enumerate(hs):\n",
        "            semantic_embeddings.append(self.gat(h, adj[i]))\n",
        "        semantic_embeddings = torch.stack(semantic_embeddings)\n",
        "\n",
        "        \n",
        "        return semantic_embeddings                \n",
        "\n",
        "class SiamHAN(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_vals, max_types, max_paths, max_pathlen, threshold=0.9):\n",
        "      \n",
        "        super().__init__()\n",
        "        self.dropout = 0.6\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.n_neighbours = max_types\n",
        "        self.max_types = nn.Parameter(torch.DoubleTensor([max_types]))\n",
        "        self.max_types.requires_grad = False\n",
        "        self.max_paths = max_paths\n",
        "        self.max_pathlen = max_pathlen\n",
        "        self.embedding_dim = np.array(emb_vals).shape[1]\n",
        "       \n",
        "        self.threshold = nn.Parameter(torch.DoubleTensor([threshold]))\n",
        "        self.threshold.requires_grad = False\n",
        "        self.gnn = HANLayer(self.embedding_dim, self.embedding_dim, self.dropout)\n",
        "        \n",
        "        ### Embedding layer\n",
        "        self.name_embedding = nn.Embedding(len(emb_vals), self.embedding_dim)\n",
        "        self.name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(emb_vals))})\n",
        "        self.name_embedding.weight.requires_grad = False\n",
        "        ## output layer\n",
        "        self.output = nn.Linear(2*self.embedding_dim, 300)\n",
        "        ## similarity score\n",
        "        self.cosine_sim_layer = nn.CosineSimilarity(dim=1)\n",
        "        \n",
        "        self.weight = nn.Parameter(torch.DoubleTensor([1.0]))\n",
        "\n",
        "        self.weight_prop = nn.Parameter(torch.DoubleTensor([0.33]))\n",
        "        self.v = nn.Parameter(torch.DoubleTensor([1/(self.max_pathlen+1) for i in range(self.max_pathlen+1)]))\n",
        "  \n",
        "  \n",
        "    def forward(self, nodes, features, prop_nodes, prop_features, max_prop_len):\n",
        "        # define here again for ontologies\n",
        "        # get node and feature embedding\n",
        "        results = []\n",
        "        \n",
        "        self.max_prop_len = max_prop_len\n",
        "        nodes = nodes.permute(1,0) # dim: (2, batch_size)\n",
        "        features = features.permute(1,0,2,3,4) # dim: (2, batch_size, max_types, max_paths, max_pathlen)\n",
        " \n",
        "        for i in range(2):\n",
        "            node_emb = self.name_embedding(nodes[i]) # dim: (2, batch_size)\n",
        "            feature_emb = self.name_embedding(features[i]) #  dim: (2, batch_size, max_types, max_paths, max_pathlen, 512)\n",
        "         \n",
        "            h_primes = []\n",
        "\n",
        "            for j, _node_emb in enumerate(node_emb):\n",
        "                h, adj = self.create_adjacency_and_homgraph(feature_emb[j], _node_emb)\n",
        "                semantic_attention_out = self.gnn(h, adj)\n",
        "                semantic_attention_out = torch.sum((self.v[None,:,None] * semantic_attention_out), dim=1)\n",
        "                if self.n_neighbours == 2:\n",
        "                    h_prime = self.weight*semantic_attention_out[0,:]\\\n",
        "                        + self.weight*semantic_attention_out[1,:]\n",
        "                    h_primes.append(h_prime)\n",
        "\n",
        "                if self.n_neighbours == 3:\n",
        "                    h_prime = self.weight*semantic_attention_out[0,:]\\\n",
        "                        + self.weight*semantic_attention_out[1,:]\\\n",
        "                        +self.weight*semantic_attention_out[2,:]\n",
        "                    h_primes.append(h_prime)\n",
        "                if self.n_neighbours == 4:\n",
        "                    h_prime = self.weight*semantic_attention_out[0,:]\\\n",
        "                        + self.weight*semantic_attention_out[1,:]\\\n",
        "                        +self.weight*semantic_attention_out[2,:]\\\n",
        "                        +self.weight*semantic_attention_out[3,:]\n",
        "                    h_primes.append(h_prime)\n",
        "                if self.n_neighbours == 5:\n",
        "                    h_prime = self.weight*semantic_attention_out[0,:]\\\n",
        "                        + self.weight*semantic_attention_out[1,:]\\\n",
        "                        +self.weight*semantic_attention_out[2,:]\\\n",
        "                        +self.weight*semantic_attention_out[3,:]\\\n",
        "                        +self.weight*semantic_attention_out[4,:]\n",
        "                    h_primes.append(h_prime)\n",
        "            h_primes = torch.stack(h_primes)\n",
        "            context = torch.cat((node_emb, h_primes), dim=1)\n",
        "            contextual = self.output(context)\n",
        "            results.append(contextual)\n",
        "        ## consine similarity layer which computes the similarity score\n",
        "                \n",
        "        sim_ent = self.cosine_sim_layer(results[0], results[1])\n",
        "\n",
        "        if prop_nodes.nelement() != 0:\n",
        "            # Calculate prop sum\n",
        "            aggregated_prop_sum = torch.sum(self.name_embedding(prop_features), dim=-2)\n",
        "            sim_prop = self.weight_prop * self.cosine_sim_layer(aggregated_prop_sum[:,0,0], aggregated_prop_sum[:,1,0])\n",
        "            sim_prop += self.weight_prop * self.cosine_sim_layer(aggregated_prop_sum[:,0,1], aggregated_prop_sum[:,1,1])\n",
        "            sim_prop += (1.0-2*self.weight_prop) * self.cosine_sim_layer(aggregated_prop_sum[:,0,2], aggregated_prop_sum[:,1,2])\n",
        " \n",
        "            \n",
        "\n",
        "            return torch.cat((sim_ent, sim_prop))\n",
        "            \n",
        "        return sim_ent\n",
        "\n",
        "    def create_adjacency_and_homgraph(self, features_list,  node):\n",
        "\n",
        "        adj, hs = [], []\n",
        "\n",
        "        for _, features in enumerate(features_list):\n",
        "            h, g =  self.create_graph_freatures(features, node)\n",
        "            hs.append(h)\n",
        "            adj.append(g)\n",
        "        return hs, adj\n",
        "\n",
        "    def create_adj(self, size):\n",
        "\n",
        "        adj = np.zeros(shape=(size, size))\n",
        "        adj[0,:] = 1.0\n",
        "\n",
        "        return adj\n",
        "\n",
        "    def create_graph_freatures(self, features, c_node):\n",
        "        h = []\n",
        "        values = [feature.detach().to(\"cpu\").numpy()[0,0] for feature in features]\n",
        "        if not all(np.array(values) == 0):\n",
        "            h.append(c_node.detach().to(\"cpu\").numpy())\n",
        "            \n",
        "            for i, feature in enumerate(features):\n",
        "                for j, feature_node in enumerate(feature):\n",
        "                    h.append(feature_node.detach().to(\"cpu\").numpy())\n",
        "                break\n",
        "        else:\n",
        "            for j in range(self.max_pathlen+1):\n",
        "                h.append(np.zeros(self.embedding_dim))\n",
        "        adj = self.create_adj(len(h))\n",
        "        h_tensor = torch.DoubleTensor(h).to(self.device)\n",
        "        return h_tensor, adj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqiBMswUtxzY"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckUkVhsdtz7N",
        "outputId": "a7a1cf3f-1f03-4898-b64d-c8755e84c817"
      },
      "source": [
        "## Trainer Codes\n",
        "import configparser, logging, random, sys, pickle\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from math import ceil\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Load reference alignments \n",
        "ontologies_in_alignment = []\n",
        "def load_alignments(folder):\n",
        "    global ontologies_in_alignment\n",
        "    alignments = []\n",
        "    for f in os.listdir(folder):\n",
        "        doc = minidom.parse(folder + f)\n",
        "        ls = list(zip(doc.getElementsByTagName('entity1'), doc.getElementsByTagName('entity2')))\n",
        "        src = train_folder + doc.getElementsByTagName('Ontology')[0].getAttribute(\"rdf:about\").split(\"/\")[-1].rsplit(\".\", 1)[0] + \".owl\"\n",
        "        targ = train_folder + doc.getElementsByTagName('Ontology')[1].getAttribute(\"rdf:about\").split(\"/\")[-1].rsplit(\".\", 1)[0] + \".owl\"\n",
        "        ontologies_in_alignment.append((src, targ))\n",
        "        alignments.extend([(a.getAttribute('rdf:resource'), b.getAttribute('rdf:resource')) for (a,b) in ls])\n",
        "    return alignments\n",
        "\n",
        "prefix_path = \"drive/MyDrive/Thesis_OM/\"#\"/\".join(os.path.dirname(os.path.abspath(__file__)).split(\"/\")[:-1]) + \"/\"\n",
        "\n",
        "print (\"Prefix path: \", prefix_path)\n",
        "\n",
        "# Read `config.ini` and initialize parameter values\n",
        "config = configparser.ConfigParser()\n",
        "config.read(prefix_path+'config.ini')\n",
        "\n",
        "# Initialize variables from config\n",
        "\n",
        "quick_mode = str(config[\"General\"][\"quick_mode\"])\n",
        "\n",
        "K = int(config[\"General\"][\"K\"])\n",
        "ontology_split = str(config[\"General\"][\"ontology_split\"]) == \"True\"\n",
        "max_false_examples = int(config[\"General\"][\"max_false_examples\"])\n",
        "\n",
        "alignment_folder = prefix_path + \"datasets/\" + str(config[\"General\"][\"dataset\"]) + \"/alignments/\"\n",
        "train_folder = prefix_path + \"datasets/\" + str(config[\"General\"][\"dataset\"]) + \"/ontologies/\"\n",
        "cached_embeddings_path = prefix_path + str(config[\"Paths\"][\"embedding_cache_path\"])\n",
        "model_path = prefix_path + str(config[\"Paths\"][\"save_model_path\"])\n",
        "\n",
        "spellcheck = config[\"Preprocessing\"][\"has_spellcheck\"] == \"True\"\n",
        "\n",
        "max_paths = int(config[\"Parameters\"][\"max_paths\"])\n",
        "max_pathlen = int(config[\"Parameters\"][\"max_pathlen\"])\n",
        "bag_of_neighbours = config[\"Parameters\"][\"bag_of_neighbours\"] == \"True\"\n",
        "weighted_sum = config[\"Parameters\"][\"weighted_sum\"] == \"True\"\n",
        "\n",
        "lr = float(config[\"Hyperparameters\"][\"lr\"])\n",
        "num_epochs = int(config[\"Hyperparameters\"][\"num_epochs\"])\n",
        "weight_decay = float(config[\"Hyperparameters\"][\"weight_decay\"])\n",
        "batch_size = int(config[\"Hyperparameters\"][\"batch_size\"])\n",
        "\n",
        "reference_alignments = load_alignments(alignment_folder)\n",
        "gt_mappings = [tuple([elem.split(\"/\")[-1] for elem in el]) for el in reference_alignments]\n",
        "gt_mappings = [tuple([el.split(\"#\")[0].rsplit(\".\", 1)[0] +  \"#\" +  el.split(\"#\")[1] for el in tup]) for tup in gt_mappings]\n",
        "print (\"Ontologies being aligned are: \", ontologies_in_alignment)\n",
        "\n",
        "# Preprocessing and parsing input data for training\n",
        "preprocessing = GraphParser(ontologies_in_alignment, reference_alignments)\n",
        "data_ent, data_prop, emb_indexer_new, emb_indexer_inv_new, emb_vals_new, neighbours_dicts_ent, neighbours_dicts_prop, max_types = preprocessing.process(spellcheck=True)\n",
        "        \n",
        "if os.path.isfile(cached_embeddings_path):\n",
        "    print(\"Found cached embeddings...\")\n",
        "    emb_indexer_cached, emb_indexer_inv_cached, emb_vals_cached = pickle.load(open(cached_embeddings_path, \"rb\"))\n",
        "else:\n",
        "    emb_indexer_cached, emb_indexer_inv_cached, emb_vals_cached = {}, {}, []\n",
        "\n",
        "emb_vals, emb_indexer, emb_indexer_inv = list(emb_vals_cached), dict(emb_indexer_cached), dict(emb_indexer_inv_cached)\n",
        "\n",
        "s = set(emb_indexer.keys())\n",
        "idx = len(emb_indexer_inv)\n",
        "for term in emb_indexer_new:\n",
        "    if term not in s:\n",
        "        emb_indexer[term] = idx\n",
        "        emb_indexer_inv[idx] = term\n",
        "        emb_vals.append(emb_vals_new[emb_indexer_new[term]])\n",
        "        idx += 1\n",
        "\n",
        "direct_inputs, direct_targets = [], []\n",
        "threshold_results = {}\n",
        "\n",
        "def optimize_threshold():\n",
        "    '''\n",
        "    Function to optimise threshold on validation set.\n",
        "    Calculates performance metrics (precision, recall, F1-score, F2-score, F0.5-score) for a\n",
        "    range of thresholds, dictated by the range of scores output by the model, with step size \n",
        "    0.001 and updates `threshold_results` which is the relevant dictionary.\n",
        "    '''\n",
        "    global val_data_t_ent, val_data_f_ent, threshold_results, batch_size, direct_inputs, direct_targets\n",
        "    all_results = OrderedDict()\n",
        "    direct_inputs, direct_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        all_pred = []\n",
        "        \n",
        "        np.random.shuffle(val_data_t_ent)\n",
        "        np.random.shuffle(val_data_f_ent)\n",
        "\n",
        "        np.random.shuffle(val_data_t_prop)\n",
        "        np.random.shuffle(val_data_f_prop)\n",
        "\n",
        "        # Create two sets of inputs: one for entities and one for properties\n",
        "        inputs_pos, nodes_pos, targets_pos = generate_input(val_data_t_ent, 1, neighbours_dicts_ent)\n",
        "        inputs_neg, nodes_neg, targets_neg = generate_input(val_data_f_ent, 0, neighbours_dicts_ent)\n",
        "        inputs_pos_prop, nodes_pos_prop, targets_pos_prop = generate_input(val_data_t_prop, 1, neighbours_dicts_prop)\n",
        "        inputs_neg_prop, nodes_neg_prop, targets_neg_prop = generate_input(val_data_f_prop, 0, neighbours_dicts_prop)\n",
        "\n",
        "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
        "        targets_all = list(targets_pos) + list(targets_neg)\n",
        "        nodes_all = list(nodes_pos) + list(nodes_neg)\n",
        "        \n",
        "        all_inp = list(zip(inputs_all, targets_all, nodes_all))\n",
        "        all_inp_shuffled = random.sample(all_inp, len(all_inp))\n",
        "        inputs_all, targets_all, nodes_all = list(zip(*all_inp_shuffled))\n",
        "\n",
        "        inputs_all_prop = list(inputs_pos_prop) + list(inputs_neg_prop)\n",
        "        targets_all_prop = list(targets_pos_prop) + list(targets_neg_prop)\n",
        "        nodes_all_prop = list(nodes_pos_prop) + list(nodes_neg_prop)\n",
        "        \n",
        "        all_inp_prop = list(zip(inputs_all_prop, targets_all_prop, nodes_all_prop))\n",
        "        all_inp_shuffled_prop = random.sample(all_inp_prop, len(all_inp_prop))\n",
        "        if all_inp_shuffled_prop:\n",
        "            inputs_all_prop, targets_all_prop, nodes_all_prop = list(zip(*all_inp_shuffled_prop))\n",
        "        else:\n",
        "            inputs_all_prop, targets_all_prop, nodes_all_prop = [], [], []\n",
        "\n",
        "        if len(inputs_all_prop) == 0:\n",
        "            max_prop_len = 0\n",
        "        else:\n",
        "            max_prop_len = np.max([[[len(elem) for elem in prop] for prop in elem_pair] \n",
        "            for elem_pair in inputs_all_prop])\n",
        "\n",
        "        batch_size = min(batch_size, len(inputs_all))\n",
        "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
        "        batch_size_prop = int(ceil(len(inputs_all_prop)/num_batches))\n",
        "        \n",
        "        for batch_idx in range(num_batches):\n",
        "            batch_start = batch_idx * batch_size\n",
        "            batch_end = (batch_idx+1) * batch_size\n",
        "            batch_start_prop = batch_idx * batch_size_prop\n",
        "            batch_end_prop = (batch_idx+1) * batch_size_prop\n",
        "\n",
        "            inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))\n",
        "            targets = np.array(targets_all[batch_start: batch_end])\n",
        "            nodes = np.array(nodes_all[batch_start: batch_end])\n",
        "\n",
        "            inputs_prop = np.array(pad_prop(inputs_all_prop[batch_start_prop: batch_end_prop], max_prop_len))\n",
        "            targets_prop = np.array(targets_all_prop[batch_start_prop: batch_end_prop])\n",
        "            nodes_prop = np.array(nodes_all_prop[batch_start_prop: batch_end_prop])\n",
        "            \n",
        "            targets = np.concatenate((targets, targets_prop), axis=0)\n",
        "\n",
        "            inp_elems = torch.LongTensor(inputs).to(device)\n",
        "            node_elems = torch.LongTensor(nodes).to(device)\n",
        "            targ_elems = torch.DoubleTensor(targets).to(device)\n",
        "\n",
        "            inp_props = torch.LongTensor(inputs_prop).to(device)\n",
        "            node_props = torch.LongTensor(nodes_prop).to(device)\n",
        "\n",
        "            # Run model on entities and properties \n",
        "            outputs = model(node_elems, inp_elems, node_props, inp_props, max_prop_len)\n",
        "            outputs = [el.item() for el in outputs]\n",
        "            targets = [True if el.item() else False for el in targets]\n",
        "\n",
        "            for idx, pred_elem in enumerate(outputs):\n",
        "                if idx < len(nodes):\n",
        "                    ent1 = emb_indexer_inv[nodes[idx][0]]\n",
        "                    ent2 = emb_indexer_inv[nodes[idx][1]]\n",
        "                else:\n",
        "                    ent1 = emb_indexer_inv[nodes_prop[idx-len(nodes)][0]]\n",
        "                    ent2 = emb_indexer_inv[nodes_prop[idx-len(nodes)][1]]\n",
        "                if (ent1, ent2) in all_results:\n",
        "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
        "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
        "        \n",
        "        direct_targets = [True if el else False for el in direct_targets]\n",
        "        \n",
        "        print (\"Len (direct inputs): \", len(direct_inputs))\n",
        "        for idx, direct_input in enumerate(direct_inputs):\n",
        "            ent1 = emb_indexer_inv[direct_input[0]]\n",
        "            ent2 = emb_indexer_inv[direct_input[1]]\n",
        "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
        "            all_results[(ent1, ent2)] = (round(sim, 3), direct_targets[idx])\n",
        "        \n",
        "        # Low threshold is lowest value output by model and high threshold is the highest value\n",
        "        low_threshold = round(np.min([el[0] for el in all_results.values()]) - 0.02, 3)\n",
        "        high_threshold = round(np.max([el[0] for el in all_results.values()]) + 0.02, 3)\n",
        "        threshold = low_threshold\n",
        "        step = 0.001\n",
        "\n",
        "        if not val_data_t_prop:\n",
        "            val_data_t_tot = val_data_t_ent\n",
        "        else:\n",
        "            val_data_t_tot = [tuple(pair) for pair in np.concatenate((val_data_t_ent, val_data_t_prop), axis=0)]\n",
        "        # Iterate over every threshold with step size of 0.001 and calculate all evaluation metrics\n",
        "        while threshold < high_threshold:\n",
        "            threshold = round(threshold, 3)\n",
        "            res = []\n",
        "            for i,key in enumerate(all_results):\n",
        "                if all_results[key][0] > threshold:\n",
        "                    res.append(key)\n",
        "            s = set(res)\n",
        "            fn_list = [(key, all_results[key][0]) for key in val_data_t_tot if key not in s]\n",
        "            fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
        "            tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
        "            \n",
        "            tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
        "            exception = False\n",
        "            \n",
        "            try:\n",
        "                precision = tp/(tp+fp)\n",
        "                recall = tp/(tp+fn)\n",
        "                f1score = 2 * precision * recall / (precision + recall)\n",
        "                f2score = 5 * precision * recall / (4 * precision + recall)\n",
        "                f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
        "            except:\n",
        "                exception = True\n",
        "                step = 0.001\n",
        "                threshold += step\n",
        "                continue\n",
        "\n",
        "            if threshold in threshold_results:\n",
        "                threshold_results[threshold].append([precision, recall, f1score, f2score, f0_5score])\n",
        "            else:\n",
        "                threshold_results[threshold] = [[precision, recall, f1score, f2score, f0_5score]]\n",
        "            threshold += step\n",
        "\n",
        " \n",
        "\n",
        "def is_valid(test_onto, key):\n",
        "    return tuple([el.split(\"#\")[0] for el in key]) not in test_onto\n",
        "\n",
        "def generate_data_neighbourless(elem_tuple):\n",
        "    return [emb_indexer[elem] for elem in elem_tuple]\n",
        "\n",
        "def embedify(seq):\n",
        "    for item in seq:\n",
        "        if isinstance(item, list):\n",
        "            yield list(embedify(item))\n",
        "        else:\n",
        "            yield emb_indexer[item]\n",
        "\n",
        "def pad_prop(inputs, max_prop_len):\n",
        "    inputs_padded = [[[elem + [0 for i in range(max_prop_len - len(elem))]\n",
        "                         for elem in prop]\n",
        "                    for prop in elem_pair]\n",
        "                for elem_pair in inputs]\n",
        "    return inputs_padded\n",
        "\n",
        "def generate_data(elem_tuple, neighbours_dicts):\n",
        "    return list(embedify([neighbours_dicts[elem] for elem in elem_tuple]))\n",
        "\n",
        "def to_feature(inputs):\n",
        "    inputs_lenpadded = [[[[path[:max_pathlen] + [0 for i in range(max_pathlen -len(path[:max_pathlen]))]\n",
        "                                    for path in nbr_type[:max_paths]]\n",
        "                                for nbr_type in ent[:max_types]]\n",
        "                            for ent in elem]\n",
        "                        for elem in inputs]\n",
        "    inputs_pathpadded = [[[nbr_type + [[0 for j in range(max_pathlen)]\n",
        "                             for i in range(max_paths - len(nbr_type))]\n",
        "                            for nbr_type in ent] for ent in elem]\n",
        "                        for elem in inputs_lenpadded]\n",
        "    return inputs_pathpadded\n",
        "\n",
        "def generate_input(elems, target, neighbours_dicts):\n",
        "    inputs, targets, nodes = [], [], []\n",
        "    global direct_inputs, direct_targets\n",
        "    for elem in list(elems):\n",
        "        try:\n",
        "            inputs.append(generate_data(elem, neighbours_dicts))\n",
        "            nodes.append(generate_data_neighbourless(elem))\n",
        "            targets.append(target)\n",
        "        except KeyError as e:\n",
        "            direct_inputs.append(generate_data_neighbourless(elem))\n",
        "            direct_targets.append(target)\n",
        "        except Exception as e:\n",
        "            raise\n",
        "    return inputs, nodes, targets\n",
        "\n",
        "def count_non_unk(elem):\n",
        "    return len([l for l in elem if l!=\"<UNK>\"])\n",
        "\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "data_items = data_ent.items()\n",
        "np.random.shuffle(list(data_items))\n",
        "data_ent = OrderedDict(data_items)\n",
        "\n",
        "data_items = data_prop.items()\n",
        "np.random.shuffle(list(data_items))\n",
        "data_prop = OrderedDict(data_items)\n",
        "\n",
        "print (\"Number of entity pairs:\", len(data_ent))\n",
        "print (\"Number of property pairs:\", len(data_prop))\n",
        "\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "index=0\n",
        "ontologies_in_alignment = [tuple([elem.split(\"/\")[-1].split(\".\")[0] for elem in pair]) for pair in ontologies_in_alignment]\n",
        "if ontology_split:\n",
        "    # We split on the ontology-pair level\n",
        "    step = int(len(ontologies_in_alignment)/K)\n",
        "    \n",
        "    val_onto = ontologies_in_alignment[len(ontologies_in_alignment)-step+1:]\n",
        "    train_data_ent = {elem: data_ent[elem] for elem in data_ent if tuple([el.split(\"#\")[0] for el in elem]) not in val_onto}\n",
        "    val_data_ent = {elem: data_ent[elem] for elem in data_ent if tuple([el.split(\"#\")[0] for el in elem]) in val_onto}\n",
        "    \n",
        "    train_data_prop = {elem: data_prop[elem] for elem in data_prop if tuple([el.split(\"#\")[0] for el in elem]) not in val_onto}\n",
        "    val_data_prop = {elem: data_prop[elem] for elem in data_prop if tuple([el.split(\"#\")[0] for el in elem]) in val_onto}\n",
        "    \n",
        "    train_data_t_ent = [key for key in train_data_ent if train_data_ent[key]]\n",
        "    train_data_f_ent = [key for key in train_data_ent if not train_data_ent[key]]\n",
        "\n",
        "    train_data_t_prop = [key for key in train_data_prop if train_data_prop[key]]\n",
        "    train_data_f_prop = [key for key in train_data_prop if not train_data_prop[key]]\n",
        "\n",
        "    val_data_t_ent = [key for key in val_data_ent if val_data_ent[key]]\n",
        "    val_data_f_ent = [key for key in val_data_ent if not val_data_ent[key]]\n",
        "\n",
        "    val_data_t_prop = [key for key in val_data_prop if val_data_prop[key]]\n",
        "    val_data_f_prop = [key for key in val_data_prop if not val_data_prop[key]]\n",
        "\n",
        "else:\n",
        "    # We split on the mapping-pair level\n",
        "    ratio = float(1/K)\n",
        "    data_t_ent = {elem: data_ent[elem] for elem in data_ent if data_ent[elem]}\n",
        "    data_f_ent = {elem: data_ent[elem] for elem in data_ent if not data_ent[elem]}\n",
        "\n",
        "    data_t_prop = {elem: data_prop[elem] for elem in data_prop if data_prop[elem]}\n",
        "    data_f_prop = {elem: data_prop[elem] for elem in data_prop if not data_prop[elem]}\n",
        "\n",
        "    data_t_items_ent = list(data_t_ent.keys())\n",
        "    data_f_items_ent = list(data_f_ent.keys())\n",
        "\n",
        "    data_t_items_prop = list(data_t_prop.keys())\n",
        "    data_f_items_prop = list(data_f_prop.keys())\n",
        "\n",
        "    val_data_t_ent = data_t_items_ent[int((ratio*index)*len(data_t_ent)):int((ratio*index + ratio)*len(data_t_ent))]\n",
        "    val_data_f_ent = data_f_items_ent[int((ratio*index)*len(data_f_ent)):int((ratio*index + ratio)*len(data_f_ent))]\n",
        "\n",
        "    train_data_t_ent = data_t_items_ent[:int(ratio*index*len(data_t_ent))] + data_t_items_ent[int(ratio*(index+1)*len(data_t_ent)):]\n",
        "    train_data_f_ent = data_f_items_ent[:int(ratio*index*len(data_f_ent))] + data_f_items_ent[int(ratio*(index+1)*len(data_f_ent)):]\n",
        "\n",
        "    val_data_t_prop = data_t_items_prop[int((ratio*index)*len(data_t_prop)):int((ratio*index + ratio)*len(data_t_prop))]\n",
        "    val_data_f_prop = data_f_items_prop[int((ratio*index)*len(data_f_prop)):int((ratio*index + ratio)*len(data_f_prop))]\n",
        "\n",
        "    train_data_t_prop = data_t_items_prop[:int(ratio*index*len(data_t_prop))] + data_t_items_prop[int(ratio*(index+1)*len(data_t_prop)):]\n",
        "    train_data_f_prop = data_f_items_prop[:int(ratio*index*len(data_f_prop))] + data_f_items_prop[int(ratio*(index+1)*len(data_f_prop)):]\n",
        "\n",
        "np.random.shuffle(train_data_f_ent)\n",
        "train_data_f_ent = train_data_f_ent[:max_false_examples]\n",
        "\n",
        "np.random.shuffle(train_data_f_prop)\n",
        "train_data_f_prop = train_data_f_prop[:max_false_examples]\n",
        "\n",
        "# Oversampling to maintain 1:1 ratio between positives and negatives\n",
        "train_data_t_ent = np.repeat(train_data_t_ent, ceil(len(train_data_f_ent)/len(train_data_t_ent)), axis=0)\n",
        "train_data_t_ent = train_data_t_ent[:len(train_data_f_ent)].tolist()\n",
        "\n",
        "if train_data_t_prop:\n",
        "    train_data_t_prop = np.repeat(train_data_t_prop, ceil(len(train_data_f_prop)/len(train_data_t_prop)), axis=0)\n",
        "    train_data_t_prop = train_data_t_prop[:len(train_data_f_prop)].tolist()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = SiamHAN(emb_vals, max_types, max_paths, max_pathlen).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "print (\"Starting training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    inputs_pos_ent, nodes_pos_ent, targets_pos_ent = generate_input(train_data_t_ent, 1, neighbours_dicts_ent)\n",
        "    inputs_neg_ent, nodes_neg_ent, targets_neg_ent = generate_input(train_data_f_ent, 0, neighbours_dicts_ent)\n",
        "    inputs_pos_prop, nodes_pos_prop, targets_pos_prop = generate_input(train_data_t_prop, 1, neighbours_dicts_prop)\n",
        "    inputs_neg_prop, nodes_neg_prop, targets_neg_prop = generate_input(train_data_f_prop, 0, neighbours_dicts_prop)\n",
        "\n",
        "    inputs_all_ent = list(inputs_pos_ent) + list(inputs_neg_ent)\n",
        "    targets_all_ent = list(targets_pos_ent) + list(targets_neg_ent)\n",
        "    nodes_all_ent = list(nodes_pos_ent) + list(nodes_neg_ent)\n",
        "    \n",
        "    all_inp_ent = list(zip(inputs_all_ent, targets_all_ent, nodes_all_ent))\n",
        "    all_inp_shuffled_ent = random.sample(all_inp_ent, len(all_inp_ent))\n",
        "    inputs_all_ent, targets_all_ent, nodes_all_ent = list(zip(*all_inp_shuffled_ent))\n",
        "\n",
        "    inputs_all_prop = list(inputs_pos_prop) + list(inputs_neg_prop)\n",
        "    targets_all_prop = list(targets_pos_prop) + list(targets_neg_prop)\n",
        "    nodes_all_prop = list(nodes_pos_prop) + list(nodes_neg_prop)\n",
        "\n",
        "    if len(inputs_all_prop) == 0:\n",
        "        max_prop_len = 0\n",
        "    else:\n",
        "        max_prop_len = np.max([[[len(elem) for elem in prop] for prop in elem_pair] \n",
        "        for elem_pair in inputs_all_prop])\n",
        "    \n",
        "    all_inp_prop = list(zip(inputs_all_prop, targets_all_prop, nodes_all_prop))\n",
        "    all_inp_shuffled_prop = random.sample(all_inp_prop, len(all_inp_prop))\n",
        "    if all_inp_shuffled_prop:\n",
        "        inputs_all_prop, targets_all_prop, nodes_all_prop = list(zip(*all_inp_shuffled_prop))\n",
        "    else:\n",
        "        inputs_all_prop, targets_all_prop, nodes_all_prop = [], [], []\n",
        "\n",
        "    batch_size = min(batch_size, len(inputs_all_ent))\n",
        "    num_batches = int(ceil(len(inputs_all_ent)/batch_size))\n",
        "    batch_size_prop = int(ceil(len(inputs_all_prop)/num_batches))\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        batch_start_ent = batch_idx * batch_size\n",
        "        batch_end_ent = (batch_idx+1) * batch_size\n",
        "        batch_start_prop = batch_idx * batch_size_prop\n",
        "        batch_end_prop = (batch_idx+1) * batch_size_prop\n",
        "        \n",
        "        inputs_ent = np.array(to_feature(inputs_all_ent[batch_start_ent: batch_end_ent]))\n",
        "        targets_ent = np.array(targets_all_ent[batch_start_ent: batch_end_ent])\n",
        "        nodes_ent = np.array(nodes_all_ent[batch_start_ent: batch_end_ent])\n",
        "\n",
        "        inputs_prop = np.array(pad_prop(inputs_all_prop[batch_start_prop: batch_end_prop], max_prop_len))\n",
        "        targets_prop = np.array(targets_all_prop[batch_start_prop: batch_end_prop])\n",
        "        nodes_prop = np.array(nodes_all_prop[batch_start_prop: batch_end_prop])\n",
        "        \n",
        "        targets = np.concatenate((targets_ent, targets_prop), axis=0)\n",
        "        \n",
        "        inp_elems = torch.LongTensor(inputs_ent).to(device)\n",
        "        node_elems = torch.LongTensor(nodes_ent).to(device)\n",
        "        targ_elems = torch.DoubleTensor(targets).to(device)\n",
        "\n",
        "        inp_props = torch.LongTensor(inputs_prop).to(device)\n",
        "        node_props = torch.LongTensor(nodes_prop).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(node_elems, inp_elems, node_props, inp_props, max_prop_len)\n",
        "\n",
        "        loss = F.mse_loss(outputs, targ_elems)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx%5000 == 0:\n",
        "            print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))  \n",
        "\n",
        "print (\"Training complete!\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print (\"Optimizing threshold...\")\n",
        "optimize_threshold()\n",
        "\n",
        "threshold_results_mean = {el: np.mean(threshold_results[el], axis=0) for el in threshold_results}    \n",
        "threshold = max(threshold_results_mean.keys(), key=(lambda key: threshold_results_mean[key][2]))\n",
        "print(threshold)\n",
        "model.threshold=nn.Parameter(torch.DoubleTensor([threshold]))\n",
        "\n",
        "torch.save(model.state_dict(), prefix_path+\"model_2.pt\")\n",
        "\n",
        "print (\"Done. Saved model at {}\".format(model_path))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prefix path:  drive/MyDrive/Thesis_OM/\n",
            "Ontologies being aligned are:  [('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/cmt.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/confOf.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/conference.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/iasted.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/edas.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/sigkdd.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/edas.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/iasted.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/edas.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/ekaw.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/confOf.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/edas.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/conference.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/ekaw.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/conference.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/edas.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/cmt.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/edas.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/conference.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/sigkdd.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/conference.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/confOf.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/cmt.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/ekaw.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/cmt.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/iasted.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/cmt.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/sigkdd.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/confOf.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/ekaw.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/confOf.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/iasted.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/confOf.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/sigkdd.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/ekaw.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/iasted.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/iasted.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/sigkdd.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/ekaw.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/sigkdd.owl'), ('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/cmt.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/conference.owl')]\n",
            "Number of entity pairs: 98688\n",
            "Number of property pairs: 40897\n",
            "Starting training...\n",
            "Epoch: 0 Idx: 0 Loss: 0.1918440141743759\n",
            "Epoch: 0 Idx: 5000 Loss: 0.07505819188994264\n",
            "Epoch: 1 Idx: 0 Loss: 0.03048455116668233\n",
            "Epoch: 1 Idx: 5000 Loss: 0.034600513293140865\n",
            "Epoch: 2 Idx: 0 Loss: 0.04176823101936814\n",
            "Epoch: 2 Idx: 5000 Loss: 0.03390349873965465\n",
            "Epoch: 3 Idx: 0 Loss: 0.07410353903498101\n",
            "Epoch: 3 Idx: 5000 Loss: 0.031206287243673844\n",
            "Epoch: 4 Idx: 0 Loss: 0.021356453097127713\n",
            "Epoch: 4 Idx: 5000 Loss: 0.05768894892244259\n",
            "Training complete!\n",
            "Optimizing threshold...\n",
            "Len (direct inputs):  0\n",
            "0.898\n",
            "Done. Saved model at drive/MyDrive/Thesis_OM/graph_representation/saved_models/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "342DzzcMSJsC"
      },
      "source": [
        "\n",
        "threshold = model.threshold.data.cpu().numpy()[0]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waI7h_3cUlaR"
      },
      "source": [
        "threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og2J2PBnSgRz"
      },
      "source": [
        "model.max_types.data.cpu().numpy()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEbTSmuqDrVe"
      },
      "source": [
        "pretrained_dict = torch.load(\"drive/MyDrive/Thesis_OM/model.pt\", map_location=torch.device(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3LYt5nOP5UK"
      },
      "source": [
        "pretrained_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKpspzHdia02"
      },
      "source": [
        "import itertools\n",
        "from xml.dom import minidom\n",
        "from urllib.request import urlopen\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\n",
        "# Define class to parse ontology\n",
        "class Ontology():\n",
        "    def __init__(self, ontology):\n",
        "        '''\n",
        "        Instantiates an Ontology object. \n",
        "        Args:\n",
        "            ontology: Path to ontology file\n",
        "        Returns:\n",
        "            Parsed Ontology object\n",
        "        '''\n",
        "        self.ontology = ontology\n",
        "        if ontology.startswith(\"https://\") or ontology.startswith(\"http://\"):\n",
        "            self.ontology_obj = minidom.parse(urlopen(ontology + \"/\"))\n",
        "        else:\n",
        "            self.ontology_obj = minidom.parse(ontology)\n",
        "        self.root = self.ontology_obj.documentElement # root node\n",
        "        \n",
        "        self.languages = []\n",
        "        self.construct_mapping_dict()\n",
        "        # Detects language of ontology by taking max of all lang attributes in labels\n",
        "        self.detect_language()\n",
        "        \n",
        "        # Dict that records the immediate \"subclass_of\" parent of any entity\n",
        "        self.parents_dict = {}\n",
        "\n",
        "        self.subclasses = self.parse_subclasses()\n",
        "        self.object_properties = self.parse_object_properties()\n",
        "        self.data_properties = self.parse_data_properties()\n",
        "        self.triples = self.parse_triples()\n",
        "        self.classes = self.parse_classes()        \n",
        "    \n",
        "\n",
        "    ### TODO missing codes ###\n",
        "    def construct_mapping_dict(self):\n",
        "        '''\n",
        "        Constructs ID to label mapping dict for ontologies where \n",
        "        entities are identified by IDs.\n",
        "        '''\n",
        "        elements = self.root.getElementsByTagName(\"owl:Class\") + self.root.getElementsByTagName(\"owl:ObjectProperty\") + self.root.getElementsByTagName(\"owl:DatatypeProperty\")\n",
        "        self.mapping_dict = {self.extract_ID(el, False): self.return_label(el) for el in elements if self.get_child_node(el, \"rdfs:label\")}\n",
        "        self.mapping_dict_inv = {self.mapping_dict[key]: key for key in self.mapping_dict}\n",
        "        return\n",
        "    \n",
        "    def return_label(self, el):\n",
        "        '''\n",
        "        Returns label of an element, and also detects language of the label\n",
        "        '''\n",
        "        label_element = self.get_child_node(el, \"rdfs:label\")[0]\n",
        "        lang = label_element.getAttribute(\"xml:lang\")\n",
        "        if lang:\n",
        "            self.languages.append(lang)\n",
        "        return label_element.firstChild.nodeValue\n",
        "\n",
        "    def detect_language(self):\n",
        "        if self.languages:\n",
        "            self.language = max(set(self.languages), key=self.languages.count)\n",
        "        else:\n",
        "            self.language = \"en\"\n",
        "\n",
        "    def get_child_node(self, element, tag):\n",
        "        '''\n",
        "        Returns child node with a specific tag name given DOM element \n",
        "        Args:\n",
        "            element: DOM parent element\n",
        "            tag: Name of tag of child\n",
        "        Returns:\n",
        "            DOM child element \n",
        "        '''\n",
        "        return [e for e in element._get_childNodes() if type(e)==minidom.Element and e._get_tagName() == tag]\n",
        "        \n",
        "    def has_attribute_value(self, element, attribute, value):\n",
        "        '''\n",
        "        Checks whether DOM element has attribute with a particular value\n",
        "        Args:\n",
        "            element: DOM parent element\n",
        "            attribute: Attribute ID\n",
        "            value: Required value of attribute\n",
        "        Returns:\n",
        "            boolean\n",
        "        '''\n",
        "        return True if element.getAttribute(attribute).split(\"#\")[-1] == value else False\n",
        "    \n",
        "    def get_subclass_triples(self, check_coded=False):\n",
        "        '''\n",
        "        Returns subclass triples\n",
        "        '''\n",
        "        subclasses = self.get_subclasses(check_coded)\n",
        "        return [(b,a,c,d) for (a,b,c,d) in subclasses]\n",
        "    \n",
        "    def parse_triples(self, union_flag=0, subclass_of=True, check_coded=False):\n",
        "        '''\n",
        "        Parses ontology to obtain object property, data property and subclass triples \n",
        "        Args:\n",
        "            union_flag: 0, if classes containing union of n classes are to be denoted \n",
        "            as a single class or n separate classes, else 1\n",
        "            subclass_of: Determines if subclass triples should be returned or not\n",
        "            check_coded: Determines if element should be queried for its label while extracting ID\n",
        "        Returns:\n",
        "            list of 4-tuples of the form (a,b,c,d) where d is the type of property c is.\n",
        "        '''\n",
        "        obj_props = [(prop, \"Object Property\") for prop in self.object_properties]\n",
        "        data_props = [(prop, \"Datatype Property\") for prop in self.data_properties]\n",
        "        props = obj_props + data_props\n",
        "        all_triples = []\n",
        "        for prop, prop_type in props:\n",
        "            domain_children = self.get_child_node(prop, \"rdfs:domain\")\n",
        "            range_children = self.get_child_node(prop, \"rdfs:range\")\n",
        "            domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children])\n",
        "            range_prop = self.filter_null([self.extract_ID(el) for el in range_children])\n",
        "            if not domain_children or not range_children:\n",
        "                # Either domain is not present or range is not present\n",
        "                continue\n",
        "            if not domain_prop:\n",
        "                domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children[0].getElementsByTagName(\"owl:Class\")])\n",
        "            if not range_prop:\n",
        "                range_prop = self.filter_null([self.extract_ID(el) for el in range_children[0].getElementsByTagName(\"owl:Class\")])\n",
        "            if domain_prop and range_prop:\n",
        "                if union_flag == 0:\n",
        "                    all_triples.extend([(el[0], el[1], self.extract_ID(prop), prop_type) for el in list(itertools.product(domain_prop, range_prop))])\n",
        "                else:\n",
        "                    all_triples.append((\"###\".join(domain_prop), \"###\".join(range_prop), self.extract_ID(prop), prop_type))\n",
        "        if subclass_of:\n",
        "            all_triples.extend(self.get_subclass_triples(check_coded))\n",
        "        return list(set(all_triples))\n",
        "    \n",
        "    def get_triples(self, union_flag=0, subclass_of=True, check_coded=False):\n",
        "        '''\n",
        "        Wrapper on top of parse_triples\n",
        "        '''\n",
        "        return self.parse_triples(union_flag, subclass_of, check_coded)\n",
        "\n",
        "    def parse_subclasses(self):\n",
        "        '''\n",
        "        Parses ontology to obtain subclass triples\n",
        "        Returns:\n",
        "            list of subclass triples of the form (a,b,c,d)\n",
        "        '''\n",
        "        subclasses = self.root.getElementsByTagName(\"rdfs:subClassOf\")\n",
        "        subclass_pairs = []\n",
        "        for el in subclasses:\n",
        "            inline_subclasses = self.extract_ID(el)\n",
        "            if inline_subclasses:\n",
        "                # Subclass of with inline IDs\n",
        "                subclass_pairs.append((el, el.parentNode, \"subclass_of\", \"Subclass\"))\n",
        "            else:\n",
        "                level1_class = self.get_child_node(el, \"owl:Class\")\n",
        "                if not level1_class:\n",
        "                    # Subclass of relations with owl Restrictions\n",
        "                    restriction = el.getElementsByTagName(\"owl:Restriction\")\n",
        "                    if not restriction:\n",
        "                        continue\n",
        "                    prop = self.get_child_node(restriction[0], \"owl:onProperty\")\n",
        "                    some_vals = self.get_child_node(restriction[0], \"owl:someValuesFrom\")\n",
        "                    \n",
        "                    if not prop or not some_vals:\n",
        "                        continue\n",
        "                    try:\n",
        "                        if self.extract_ID(prop[0]) and self.extract_ID(some_vals[0]):\n",
        "                            subclass_pairs.append((el.parentNode, some_vals[0], self.extract_ID(prop[0]), \"Object Property\"))\n",
        "                        elif self.extract_ID(prop[0]) and not self.extract_ID(some_vals[0]):\n",
        "                            class_vals = self.get_child_node(some_vals[0], \"owl:Class\")\n",
        "                            subclass_pairs.append((el.parentNode, class_vals[0], self.extract_ID(prop[0]), \"Object Property\"))\n",
        "                        elif not self.extract_ID(prop[0]) and self.extract_ID(some_vals[0]):\n",
        "                            prop_vals = self.get_child_node(prop[0], \"owl:ObjectProperty\")\n",
        "                            subclass_pairs.append((el.parentNode, some_vals[0], self.extract_ID(prop_vals[0]), \"Object Property\"))\n",
        "                        else:\n",
        "                            prop_vals = self.get_child_node(prop[0], \"owl:ObjectProperty\")\n",
        "                            class_vals = self.get_child_node(some_vals[0], \"owl:Class\")\n",
        "                            subclass_pairs.append((el.parentNode, class_vals[0], self.extract_ID(prop_vals[0]), \"Object Property\"))\n",
        "                    except Exception as e:\n",
        "                        try:\n",
        "                            if not self.extract_ID(prop[0]) and self.extract_ID(some_vals[0]):\n",
        "                                prop_vals = self.get_child_node(prop[0], \"owl:DatatypeProperty\")\n",
        "                                subclass_pairs.append((el.parentNode, some_vals[0], self.extract_ID(prop_vals[0]), \"Datatype Property\"))\n",
        "                            elif not self.extract_ID(prop[0]) and not self.extract_ID(some_vals[0]):\n",
        "                                prop_vals = self.get_child_node(prop[0], \"owl:DatatypeProperty\")\n",
        "                                class_vals = self.get_child_node(some_vals[0], \"owl:Class\")\n",
        "                                subclass_pairs.append((el.parentNode, class_vals[0], self.extract_ID(prop_vals[0]), \"Datatype Property\"))\n",
        "                        except Exception as e:\n",
        "                            continue\n",
        "                else:\n",
        "                    if self.extract_ID(level1_class[0]):\n",
        "                        # Subclass label under a level 1 tag\n",
        "                        subclass_pairs.append((level1_class[0], el.parentNode, \"subclass_of\", \"Subclass\"))\n",
        "                    else:\n",
        "                        continue\n",
        "        return subclass_pairs\n",
        "        \n",
        "    def get_subclasses(self, check_coded=False):\n",
        "        '''\n",
        "        Extracts entity ID from parsed subclass triples\n",
        "        '''\n",
        "        subclasses = [(self.extract_ID(a, not check_coded), self.extract_ID(b, not check_coded), c, d) for (a,b,c,d) in self.subclasses]\n",
        "        self.parents_dict = {}\n",
        "        for (a,b,c,d) in subclasses:\n",
        "            if c == \"subclass_of\" and a!=\"Thing\" and b!=\"Thing\":\n",
        "                if b not in self.parents_dict:\n",
        "                    self.parents_dict[b] = [a]\n",
        "                else:\n",
        "                    self.parents_dict[b].append(a)\n",
        "        return [el for el in subclasses if el[0] and el[1] and el[2] and el[0]!=\"Thing\" and el[1]!=\"Thing\"]\n",
        "    \n",
        "    def filter_null(self, data):\n",
        "        return [el for el in data if el]\n",
        "    \n",
        "    def extract_ns(self):\n",
        "        '''\n",
        "        Extracts namespace of an ontology\n",
        "        '''\n",
        "        ns = self.ontology_obj.getElementsByTagName(\"rdf:RDF\")[0].getAttribute(\"xmlns\")\n",
        "        if ns[-1] == \"#\":\n",
        "            return ns\n",
        "        return self.ontology_obj.doctype.entities.item(0).firstChild.nodeValue\n",
        "\n",
        "    def extract_ID(self, element, check_coded = True):\n",
        "        '''\n",
        "        Returns ID for a parsed DOM element. In ontologies where classes are represented by \n",
        "        numerical IDs, it returns the label (stored in mapping_dict)\n",
        "        '''\n",
        "        element_id = element.getAttribute(\"rdf:ID\") or element.getAttribute(\"rdf:resource\") or element.getAttribute(\"rdf:about\")\n",
        "        element_id = element_id.split(\"#\")[-1].split(\";\")[-1]\n",
        "        if len(list(filter(str.isdigit, element_id))) >= 3 and \"_\" in element_id and check_coded:\n",
        "            return self.mapping_dict[element_id]\n",
        "        return element_id.replace(\"UNDEFINED_\", \"\").replace(\"DO_\", \"\")\n",
        "\n",
        "    def parse_classes(self):\n",
        "        '''\n",
        "        Parse all entities, including domain and range entities in ontology\n",
        "        '''\n",
        "        class_elems = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
        "        subclass_classes = list(set(flatten([el[:-2] for el in self.triples])))\n",
        "        return list(set(self.filter_null(class_elems + subclass_classes)))\n",
        "    \n",
        "    def get_classes(self):\n",
        "        return self.classes\n",
        "    \n",
        "    def get_entities(self):\n",
        "        '''\n",
        "        Parse only classes\n",
        "        '''\n",
        "        entities = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
        "        return list(set(self.filter_null(entities)))\n",
        "\n",
        "    def parse_data_properties(self):\n",
        "        '''\n",
        "        Parse all datatype properties, including functional and inverse functional datatype properties\n",
        "        '''\n",
        "        data_properties = [el for el in self.get_child_node(self.root, 'owl:DatatypeProperty')]\n",
        "        fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
        "        fn_data_properties = [el for el in fn_data_properties if type(el)==minidom.Element and \n",
        "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
        "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
        "        inv_fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
        "        inv_fn_data_properties = [el for el in inv_fn_data_properties if type(el)==minidom.Element and \n",
        "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
        "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
        "        return data_properties + fn_data_properties + inv_fn_data_properties\n",
        "        \n",
        "    def parse_object_properties(self):\n",
        "        '''\n",
        "        Parse all object properties, including functional and inverse functional object properties\n",
        "        '''\n",
        "        obj_properties = [el for el in self.get_child_node(self.root, 'owl:ObjectProperty')]\n",
        "        fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
        "        fn_obj_properties = [el for el in fn_obj_properties if type(el)==minidom.Element and \n",
        "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
        "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
        "        inv_fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
        "        inv_fn_obj_properties = [el for el in inv_fn_obj_properties if type(el)==minidom.Element and \n",
        "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
        "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
        "        return obj_properties + fn_obj_properties + inv_fn_obj_properties\n",
        "    \n",
        "    def get_object_properties(self):\n",
        "        '''\n",
        "        Wrapper to obtain object properties and parse them to return property IDs \n",
        "        '''\n",
        "        obj_props = [self.extract_ID(el) for el in self.object_properties]\n",
        "        return list(set(self.filter_null(obj_props)))\n",
        "    \n",
        "    def get_data_properties(self):\n",
        "        '''\n",
        "        Wrapper to obtain data properties and parse them to return property IDs \n",
        "        '''\n",
        "        data_props = [self.extract_ID(el) for el in self.data_properties]\n",
        "        return list(set(self.filter_null(data_props)))\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtAGkgMguG8R"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpaqHM7ch2BT"
      },
      "source": [
        "onto_name_list = []\n",
        "for i, file in enumerate(os.listdir(\"drive/MyDrive/Thesis_OM/datasets/conference/alignments\")):\n",
        "  onto_name_list.append(file.split(\".\")[0].split(\"-\"))\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SSAEvWpQoVs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqbHDIdHfudC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a92c53-129d-48cd-aa79-81cbc10fa358"
      },
      "source": [
        "import configparser, sys, logging, random, os, pickle\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from math import ceil\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "for model_name in [\"model_2.pt\"]:\n",
        "\n",
        "  model_path =\"drive/MyDrive/Thesis_OM/\"+model_name\n",
        "  for ont_name1, ont_name2 in onto_name_list:\n",
        "    prefix_path_onto = \"drive/MyDrive/Thesis_OM/datasets/conference/ontologies/\"\n",
        "    ont_name1, ont_name2  = prefix_path_onto+ont_name1+\".owl\", prefix_path_onto+ont_name2+\".owl\"\n",
        "    if ont_name1.endswith(\"/\"):\n",
        "        ont_name1 = ont_name1[:-1]\n",
        "    if ont_name2.endswith(\"/\"):\n",
        "        ont_name2 = ont_name2[:-1]\n",
        "\n",
        "    prefix_path = \"drive/MyDrive/Thesis_OM/\"#\"/\".join(os.path.dirname(os.path.abspath(__file__)).split(\"/\")[:-1]) + \"/\"\n",
        "    outfolder = prefix_path+model_name.split(\".\")[0]+\"/\"\n",
        "\n",
        "\n",
        "\n",
        "    # Read `config.ini` and initialize parameter values\n",
        "    config = configparser.ConfigParser()\n",
        "    config.read(prefix_path + 'config.ini')\n",
        "\n",
        "    logging.info(\"Prefix path: \", prefix_path)\n",
        "\n",
        "    # Initialize variables from config\n",
        "    quick_mode = str(config[\"General\"][\"quick_mode\"])\n",
        "\n",
        "    #model_path = prefix_path + str(config[\"Paths\"][\"load_model_path\"])\n",
        "    #output_path = prefix_path + str(config[\"Paths\"][\"output_folder\"])\n",
        "    cached_embeddings_path = prefix_path + str(config[\"Paths\"][\"embedding_cache_path\"])\n",
        "    spellcheck = config[\"Preprocessing\"][\"has_spellcheck\"] == \"True\"\n",
        "\n",
        "    max_paths = int(config[\"Parameters\"][\"max_paths\"])\n",
        "    max_pathlen = int(config[\"Parameters\"][\"max_pathlen\"])\n",
        "    bag_of_neighbours = config[\"Parameters\"][\"bag_of_neighbours\"] == \"True\"\n",
        "    weighted_sum = config[\"Parameters\"][\"weighted_sum\"] == \"True\"\n",
        "    # print(model_settings.loc[model_settings[\"model_name\"]==model_name][\"batch_size\"])\n",
        "    batch_size =16# model_settings.loc[model_settings[\"model_name\"]==model_name][\"batch_size\"].values[0] #int(config[\"Hyperparameters\"][\"batch_size\"])\n",
        "   \n",
        "    batch_size = int(batch_size)\n",
        "    test_ontologies = [tuple([ont_name1, ont_name2])]\n",
        "    print(test_ontologies)\n",
        "    # Preprocessing and parsing input data for testing\n",
        "\n",
        "    preprocessing = GraphParser(test_ontologies)\n",
        "    test_data_ent, test_data_prop, emb_indexer_new, emb_indexer_inv_new, emb_vals_new, neighbours_dicts_ent, neighbours_dicts_prop, max_types = preprocessing.process(spellcheck)\n",
        "\n",
        "    if os.path.isfile(cached_embeddings_path):\n",
        "        logging.info(\"Found cached embeddings...\")\n",
        "        emb_indexer_cached, emb_indexer_inv_cached, emb_vals_cached = pickle.load(open(cached_embeddings_path, \"rb\"))\n",
        "    else:\n",
        "        emb_indexer_cached, emb_indexer_inv_cached, emb_vals_cached = {}, {}, []\n",
        "\n",
        "    emb_vals, emb_indexer, emb_indexer_inv = list(emb_vals_cached), dict(emb_indexer_cached), dict(emb_indexer_inv_cached)\n",
        "\n",
        "    s = set(emb_indexer.keys())\n",
        "    idx = len(emb_indexer_inv)\n",
        "    for term in emb_indexer_new:\n",
        "        if term not in s:\n",
        "            emb_indexer[term] = idx\n",
        "            emb_indexer_inv[idx] = term\n",
        "            emb_vals.append(emb_vals_new[emb_indexer_new[term]])\n",
        "            idx += 1\n",
        "\n",
        "    def count_non_unk(elem):\n",
        "        return len([l for l in elem if l!=\"<UNK>\"])\n",
        "\n",
        "    def generate_data_neighbourless(elem_tuple):\n",
        "        return [emb_indexer[elem] for elem in elem_tuple]\n",
        "\n",
        "    def embedify(seq):\n",
        "        for item in seq:\n",
        "            if isinstance(item, list):\n",
        "                yield list(embedify(item))\n",
        "            else:\n",
        "                yield emb_indexer[item]\n",
        "\n",
        "    def generate_data(elem_tuple, neighbours_dicts):\n",
        "        return list(embedify([neighbours_dicts[elem] for elem in elem_tuple]))\n",
        "\n",
        "    def to_feature(inputs):\n",
        "        inputs_lenpadded = [[[[path[:max_pathlen] + [0 for i in range(max_pathlen -len(path[:max_pathlen]))]\n",
        "                                        for path in nbr_type[:max_paths]]\n",
        "                                    for nbr_type in ent[:max_types]]\n",
        "                                for ent in elem]\n",
        "                            for elem in inputs]\n",
        "        inputs_pathpadded = [[[nbr_type + [[0 for j in range(max_pathlen)]\n",
        "                                for i in range(max_paths - len(nbr_type))]\n",
        "                                for nbr_type in ent] for ent in elem]\n",
        "                            for elem in inputs_lenpadded]\n",
        "        return inputs_pathpadded\n",
        "\n",
        "    def pad_prop(inputs):\n",
        "        inputs_padded = [[[elem + [0 for i in range(max_prop_len - len(elem))]\n",
        "                            for elem in prop]\n",
        "                        for prop in elem_pair]\n",
        "                    for elem_pair in inputs]\n",
        "        return inputs_padded\n",
        "\n",
        "    def generate_input(elems, neighbours_dicts):\n",
        "        inputs, nodes = [], []\n",
        "        \n",
        "        global direct_inputs\n",
        "        for elem in list(elems):\n",
        "            try:\n",
        "                inputs.append(generate_data(elem, neighbours_dicts))\n",
        "                nodes.append(generate_data_neighbourless(elem))\n",
        "            except Exception as e:\n",
        "                direct_inputs.append(generate_data_neighbourless(elem))\n",
        "        return inputs, nodes\n",
        "\n",
        "    def write_results():\n",
        "        ont_name_parsed1 = Ontology(ont_name1).extract_ns()\n",
        "        ont_name_parsed2 = Ontology(ont_name2).extract_ns()\n",
        "        ont_name1_pre = ont_name1 if (ont_name1.startswith(\"http://\") or ont_name1.startswith(\"https://\")) else \"file://\" + ont_name1\n",
        "        ont_name2_pre = ont_name2 if (ont_name2.startswith(\"http://\") or ont_name2.startswith(\"https://\")) else \"file://\" + ont_name2\n",
        "        rdf = \\\n",
        "        \"\"\"<?xml version='1.0' encoding='utf-8' standalone='no'?>\n",
        "    <rdf:RDF xmlns='http://knowledgeweb.semanticweb.org/heterogeneity/alignment#'\n",
        "            xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#'\n",
        "            xmlns:xsd='http://www.w3.org/2001/XMLSchema#'\n",
        "            xmlns:align='http://knowledgeweb.semanticweb.org/heterogeneity/alignment#'>\n",
        "    <Alignment>\n",
        "      <xml>yes</xml>\n",
        "      <level>0</level>\n",
        "      <type>**</type>\n",
        "      <onto1>\n",
        "        <Ontology rdf:about=\"{}\">\n",
        "          <location>{}</location>\n",
        "        </Ontology>\n",
        "      </onto1>\n",
        "      <onto2>\n",
        "        <Ontology rdf:about=\"{}\">\n",
        "          <location>{}</location>\n",
        "        </Ontology>\n",
        "      </onto2>\"\"\".format(ont_name_parsed1.split(\"#\")[0], ont_name1_pre, ont_name_parsed2.split(\"#\")[0], ont_name2_pre)\n",
        "        for (a,b,score) in final_list:\n",
        "            mapping = \"\"\"\n",
        "      <map>\n",
        "        <Cell>\n",
        "          <entity1 rdf:resource='{}'/>\n",
        "          <entity2 rdf:resource='{}'/>\n",
        "          <relation>=</relation>\n",
        "          <measure rdf:datatype='http://www.w3.org/2001/XMLSchema#float'>{}</measure>\n",
        "        </Cell>\n",
        "      </map>\"\"\".format(ont_name_parsed1 + \"#\".join(a.split(\"#\")[1:]), ont_name_parsed2 + \"#\".join(b.split(\"#\")[1:]), score)\n",
        "            rdf += mapping\n",
        "        rdf += \"\"\"\n",
        "    </Alignment>\n",
        "    </rdf:RDF>\"\"\"\n",
        "        return rdf\n",
        "\n",
        "    torch.set_default_dtype(torch.float64)\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "    random.seed(0)\n",
        "\n",
        "    np.random.shuffle(test_data_ent)\n",
        "    np.random.shuffle(test_data_prop)\n",
        "\n",
        "    torch.set_default_dtype(torch.float64)\n",
        "\n",
        "    logging.info (\"Loading trained model....\")\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    pretrained_dict = torch.load(model_path, map_location=torch.device(device))\n",
        "    \n",
        "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k!=\"name_embedding.weight\"}\n",
        "    \n",
        "    #max_types = len([key for key in pretrained_dict.keys() if key.startswith(\"w_\")]) + 1\n",
        "    max_types = int(model.max_types.data.cpu().numpy()[0])\n",
        "    model = SiamHAN(emb_vals, max_types, max_paths, max_pathlen).to(device)\n",
        "    model_dict = model.state_dict()\n",
        "\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    threshold = model.threshold.data.cpu().numpy()[0]\n",
        "    print(threshold)\n",
        "    logging.info (\"Model loaded successfully!\")\n",
        "\n",
        "    logging.info (\"Optimum Threshold: {}\".format(threshold))\n",
        "\n",
        "    model.eval()\n",
        "    logging.info (\"Length of test data(ent): {} test data(prop):{}\".format(len(test_data_ent), len(test_data_prop)))\n",
        "\n",
        "    all_results = OrderedDict()    \n",
        "    direct_inputs = []\n",
        "    with torch.no_grad():\n",
        "        inputs_all_ent, nodes_all_ent = generate_input(test_data_ent, neighbours_dicts_ent)\n",
        "        inputs_all_prop, nodes_all_prop = generate_input(test_data_prop, neighbours_dicts_prop)\n",
        "        all_inp = list(zip(inputs_all_ent, nodes_all_ent))\n",
        "        all_inp_shuffled = random.sample(all_inp, len(all_inp))\n",
        "\n",
        "        inputs_all_ent, nodes_all_ent = list(zip(*all_inp_shuffled))\n",
        "\n",
        "        all_inp = list(zip(inputs_all_prop, nodes_all_prop))\n",
        "        all_inp_shuffled = random.sample(all_inp, len(all_inp))\n",
        "        inputs_all_prop, nodes_all_prop = list(zip(*all_inp_shuffled))\n",
        "        \n",
        "        max_prop_len = np.max([[[len(elem) for elem in prop] for prop in elem_pair]\n",
        "            for elem_pair in inputs_all_prop])\n",
        "        logging.info (\"Max prop len: \", max_prop_len)\n",
        "        batch_size = min(batch_size, len(inputs_all_ent))\n",
        "        num_batches = int(ceil(len(inputs_all_ent)/batch_size))\n",
        "        batch_size_prop = int(ceil(len(inputs_all_prop)/num_batches))\n",
        "\n",
        "        logging.info (\"Num batches: {} Batch size (prop): {}\".format(num_batches, batch_size_prop))\n",
        "        for batch_idx in range(num_batches):\n",
        "            batch_start = batch_idx * batch_size\n",
        "            batch_end = (batch_idx+1) * batch_size\n",
        "            batch_start_prop = batch_idx * batch_size_prop\n",
        "            batch_end_prop = (batch_idx+1) * batch_size_prop\n",
        "            inputs_ent = np.array(to_feature(inputs_all_ent[batch_start: batch_end]))\n",
        "            nodes_ent = np.array(nodes_all_ent[batch_start: batch_end])\n",
        "\n",
        "            inputs_prop = np.array(pad_prop(inputs_all_prop[batch_start_prop: batch_end_prop]))\n",
        "            nodes_prop = np.array(nodes_all_prop[batch_start_prop: batch_end_prop])\n",
        "\n",
        "            inp_ents = torch.LongTensor(inputs_ent).to(device)\n",
        "            node_ents = torch.LongTensor(nodes_ent).to(device)\n",
        "            inp_props = torch.LongTensor(inputs_prop).to(device)\n",
        "            node_props = torch.LongTensor(nodes_prop).to(device)\n",
        "            outputs = model(node_ents, inp_ents, node_props, inp_props, max_prop_len)\n",
        "            outputs = [el.item() for el in outputs]\n",
        "\n",
        "            for idx, pred_elem in enumerate(outputs):\n",
        "                if idx < len(nodes_ent):\n",
        "                    ent1 = emb_indexer_inv[nodes_ent[idx][0]]\n",
        "                    ent2 = emb_indexer_inv[nodes_ent[idx][1]]\n",
        "                else:\n",
        "                    ent1 = emb_indexer_inv[nodes_prop[idx-len(nodes_ent)][0]]\n",
        "                    ent2 = emb_indexer_inv[nodes_prop[idx-len(nodes_ent)][1]]\n",
        "                if (ent1, ent2) in all_results:\n",
        "                    logging.info (\"Error: \", ent1, ent2, \"already present\")\n",
        "                all_results[(ent1, ent2)] = (round(pred_elem, 3), pred_elem>=threshold)\n",
        "        \n",
        "        logging.info (\"Len (direct inputs): \", len(direct_inputs))\n",
        "        for idx, direct_input in enumerate(direct_inputs):\n",
        "            ent1 = emb_indexer_inv[direct_input[0]]\n",
        "            ent2 = emb_indexer_inv[direct_input[1]]\n",
        "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
        "            all_results[(ent1, ent2)] = (round(sim, 3), pred_elem>=threshold)\n",
        "        \n",
        "    final_list = [(elem[0], elem[1], str(all_results[elem][0])) for elem in all_results if all_results[elem][1]]\n",
        "\n",
        "    ont_name_parsed1 = Ontology(ont_name1).extract_ns().split(\"/\")[-1].split(\"#\")[0].rsplit(\".\", 1)[0]\n",
        "    ont_name_parsed2 = Ontology(ont_name2).extract_ns().split(\"/\")[-1].split(\"#\")[0].rsplit(\".\", 1)[0]\n",
        "\n",
        "    f = \"HAN1-\"+ont_name_parsed1.lower() + \"-\" + ont_name_parsed2.lower() + \".rdf\"\n",
        "\n",
        "    open(outfolder + f, \"w+\").write(write_results())\n",
        "\n",
        "    logging.info (\"The final alignment file can be found below: \")\n",
        "    print(\"file://\" + outfolder + f)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/cmt.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/confOf.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-cmt-confof.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/conference.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/iasted.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-conference-iasted.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/edas.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/sigkdd.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-edas-sigkdd.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/edas.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/iasted.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-edas-iasted.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/edas.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/ekaw.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-edas-ekaw.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/confOf.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/edas.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-confof-edas.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/conference.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/ekaw.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-conference-ekaw.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/conference.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/edas.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-conference-edas.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/cmt.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/edas.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-cmt-edas.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/conference.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/sigkdd.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-conference-sigkdd.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/conference.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/confOf.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-conference-confof.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/cmt.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/ekaw.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-cmt-ekaw.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/cmt.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/iasted.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-cmt-iasted.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/cmt.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/sigkdd.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-cmt-sigkdd.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/confOf.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/ekaw.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-confof-ekaw.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/confOf.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/iasted.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-confof-iasted.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/confOf.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/sigkdd.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-confof-sigkdd.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/ekaw.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/iasted.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-ekaw-iasted.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/iasted.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/sigkdd.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-iasted-sigkdd.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/ekaw.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/sigkdd.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-ekaw-sigkdd.rdf\n",
            "[('drive/MyDrive/Thesis_OM/datasets/conference/ontologies/cmt.owl', 'drive/MyDrive/Thesis_OM/datasets/conference/ontologies/conference.owl')]\n",
            "0.898\n",
            "file://drive/MyDrive/Thesis_OM/model_2/HAN1-cmt-conference.rdf\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}